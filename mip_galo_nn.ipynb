{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docplex.mp.model import Model\n",
    "import numpy as np\n",
    "from source_torch.mlca.mlca_nn import MLCA_NN\n",
    "from source_torch.mlca.mlca_nn_mip import MLCA_NNMIP\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "import source_torch.util as util\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLCA_GALO_MIP:\n",
    "    def __init__(self, value_model, bidder_id, presampled_algorithm='unif', presampled_n=10, L = 6000):\n",
    "        self.bidder_id = bidder_id\n",
    "        self.Mip = Model(name='MLCA_GALO_MIP')\n",
    "        self.presampled_algorithm = presampled_algorithm\n",
    "        self.presampled_n = presampled_n\n",
    "        #if self.presampled_algorithm == 'gali':\n",
    "        #    D_presampled = util.gali_bids_init(value_model=value_model, bidder_id=bidder_id, n=presampled_n)\n",
    "        #else:\n",
    "        #    D_presampled = util.unif_random_bids(value_model=value_model, bidder_id=bidder_id, n=presampled_n)\n",
    "        #self.values = D_presampled[:, -1].tolist()\n",
    "        \n",
    "        #self.bundles = D_presampled[:, :-1].tolist()\n",
    "        \n",
    "        self.values = np.random.randint(1, 100, size=(self.presampled_n, 1)).tolist()\n",
    "        self.bundles = np.random.randint(0, 2, size=(self.presampled_n, 10)).tolist()\n",
    "        self.M = len(self.bundles[0])#number of items\n",
    "        self.bidder_model = self.train()\n",
    "        self.bidder_name = 'Bidder 0'\n",
    "        #test = self._get_model_layer_shapes(self.bidder_model, layer_type=['dense', 'input'])\n",
    "        self.L = L  # global big-M variable: see paper\n",
    "        print(self._get_model_layer_shapes(self.bidder_model, layer_type=['dense', 'input']))\n",
    "        #print(self._get_model_layer_shapes(self.bidder_model, layer_type=['dense', 'input']))\n",
    "        self.upper_bounds_z = {self.bidder_name: np.array([self.L]*self._get_model_layer_shapes(self.bidder_model, layer_type=['dense', 'input'])).reshape(-1, 1)}\n",
    "        self.upper_bounds_s = {self.bidder_name: np.array([self.L]*self._get_model_layer_shapes(self.bidder_model, layer_type=['dense', 'input'])).reshape(-1, 1)}\n",
    "        self.z = {}  # MIP variable: see paper\n",
    "        self.s = {}  # MIP variable: see paper\n",
    "        self.y = {}  # MIP variable: see paper\n",
    "        self.x_star = np.ones((self.N, self.M))*(-1)  # optimal allocation (-1=not yet solved)\n",
    "        self.soltime = None  # timing\n",
    "        \n",
    "                \n",
    "\n",
    "    def train(self):\n",
    "        model = MLCA_NN(X_train = np.array(self.bundles), Y_train=np.array(self.values))\n",
    "        model.initialize_model(model_parameters={'learning_rate': 0.001, 'architecture': [10, 10, 10], 'dropout': False, 'dropout_prob': 0.2, 'device': 'cpu', 'regularization_type': 'l2', 'regularization': 0.01})\n",
    "        model.fit(epochs=10, batch_size=32)\n",
    "        return model\n",
    "    \n",
    "    def _get_model_weights(model): # torch\n",
    "        nnmodel = model.model\n",
    "        weights = []\n",
    "        for params in nnmodel.parameters():\n",
    "            weights.append(params.detach().cpu().numpy().T)        \n",
    "        return weights\n",
    "\n",
    "    def _get_model_layer_shapes(model, layer_type=None):\n",
    "        ''' return layer output shapes instead, \n",
    "            if 'input' is given as desired layer type, insert input dim at the beginning.\n",
    "            assumes torch model '''\n",
    "        # nnmodel = self.Models[key]\n",
    "        nnmodel = model.model\n",
    "        Layer_shapes = []\n",
    "        for i, (name, param) in enumerate(nnmodel.named_parameters()):\n",
    "            if (i==0) and ('input' in layer_type): \n",
    "                Layer_shapes.append(param.shape[1])\n",
    "            if any([x in name for x in layer_type]) and ('bias' in name):\n",
    "                Layer_shapes.append(param.shape[0])\n",
    "        return Layer_shapes\n",
    "\n",
    "    def _clean_weights(Wb):\n",
    "        for v in range(0, len(Wb)-2, 2):\n",
    "            Wb[v][abs(Wb[v]) <= 1e-8] = 0\n",
    "            Wb[v+1][abs(Wb[v+1]) <= 1e-8] = 0\n",
    "            zero_rows = np.where(np.logical_and((Wb[v] == 0).all(axis=0), Wb[v+1] == 0))[0]\n",
    "            if len(zero_rows) > 0:\n",
    "                logging.debug('Clean Weights (rows) %s', zero_rows)\n",
    "                Wb[v] = np.delete(Wb[v], zero_rows, axis=1)\n",
    "                Wb[v+1] = np.delete(Wb[v+1], zero_rows)\n",
    "                Wb[v+2] = np.delete(Wb[v+2], zero_rows, axis=0)\n",
    "        return(Wb)\n",
    "    \n",
    "\n",
    "    def optimization_step_nn(self):\n",
    "        attempts = self.MIP_parameters['attempts_DNN_WDP']\n",
    "        for attempt in range(1, attempts+1):\n",
    "            logging.debug('Initialize MIP for GALO(NN)')\n",
    "            #X = MLCA_NNMIP(DNNs, L=self.MIP_parameters['bigM'])\n",
    "            if not self.MIP_parameters['mip_bounds_tightening']:\n",
    "                print('MIP bounds not tightening is not implemented yet')\n",
    "            elif self.MIP_parameters['mip_bounds_tightening'] == 'IA':\n",
    "                #This is what usually happens\n",
    "                self.tighten_bounds_IA(upper_bound_input=[1]*self.M)\n",
    "                #X.print_upper_bounds(only_zeros=True)\n",
    "                X.initialize_mip(verbose=False, bidder_specific_constraints=bidder_specific_constraints)\n",
    "            elif self.MIP_parameters['mip_bounds_tightening'] == 'LP':\n",
    "                print('MIP bounds tightening == LP is not implemented yet')\n",
    "                #X.tighten_bounds_IA(upper_bound_input=[1]*self.M)\n",
    "                #X.tighten_bounds_LP(upper_bound_input=[1]*self.M)\n",
    "                #X.print_upper_bounds(only_zeros=True)\n",
    "                #X.initialize_mip(verbose=False, bidder_specific_constraints=bidder_specific_constraints)\n",
    "            try:\n",
    "                logging.info('Solving MIP')\n",
    "                logging.info('Attempt no: %s', attempt)\n",
    "                if self.MIP_parameters['warm_start'] and self.warm_start_sol[economy_key] is not None:\n",
    "                    logging.debug('Using warm start')\n",
    "                    self.warm_start_sol[economy_key] = X.solve_mip(log_output=False, time_limit=self.MIP_parameters['time_limit'],\n",
    "                                                                   mip_relative_gap=self.MIP_parameters['relative_gap'], integrality_tol=self.MIP_parameters['integrality_tol'],\n",
    "                                                                   mip_start=docplex.mp.solution.SolveSolution(X.Mip, self.warm_start_sol[economy_key].as_dict()))\n",
    "                else:\n",
    "                    self.warm_start_sol[economy_key] = X.solve_mip(log_output=False, time_limit=self.MIP_parameters['time_limit'],\n",
    "                                                                   mip_relative_gap=self.MIP_parameters['relative_gap'], integrality_tol=self.MIP_parameters['integrality_tol'])\n",
    "\n",
    "                if bidder_specific_constraints is None:\n",
    "                    logging.debug('SET ARGMAX ALLOCATION FOR ALL BIDDERS')\n",
    "                    b = 0\n",
    "                    for bidder in self.argmax_allocation[economy_key].keys():\n",
    "                        self.argmax_allocation[economy_key][bidder][0] = X.x_star[b, :]\n",
    "                        b = b + 1\n",
    "                else:\n",
    "                    logging.debug('SET ARGMAX ALLOCATION ONLY BIDDER SPECIFIC for {}'.format(list(bidder_specific_constraints.keys())[0]))\n",
    "                    for bidder in bidder_specific_constraints.keys():\n",
    "                        b = X.get_bidder_key_position(bidder_key=bidder)  # transform bidder_key into bidder position in MIP\n",
    "                        self.argmax_allocation[economy_key][bidder][1] = X.x_star[b, :]  # now on position 1!\n",
    "\n",
    "                for key,value in self.argmax_allocation[economy_key].items():\n",
    "                    logging.debug(key + ':  %s | %s', value[0], value[1])\n",
    "\n",
    "                self.elapsed_time_mip[economy_key].append(X.soltime)\n",
    "                break\n",
    "            except Exception:\n",
    "                logging.warning('-----------------------------------------------')\n",
    "                logging.warning('NOT SUCCESSFULLY SOLVED in attempt: %s \\n', attempt)\n",
    "                logging.warning(X.Mip.solve_details)\n",
    "                if attempt == attempts:\n",
    "                    X.Mip.export_as_lp(basename='UnsolvedMip_iter{}_{}'.format(self.mlca_iteration, economy_key),path=os.getcwd(), hide_user_names=False)\n",
    "                    sys.exit('STOP, not solved succesfully in {} attempts\\n'.format(attempt))\n",
    "                \n",
    "                # clear_session()\n",
    "                logging.debug('REFITTING:')\n",
    "                self.estimation_step(economy_key=economy_key)\n",
    "            DNNs = OrderedDict(list((key, self.NN_models[economy_key][key].model) for key in list(self.NN_models[economy_key].keys())))\n",
    "        del X\n",
    "        del DNNs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def tighten_bounds_IA(self, upper_bound_input, bidder_model, verbose=False):\n",
    "        bidder = self.bidder_name\n",
    "        Wb_total = util._clean_weights(util._get_model_weights(bidder_model))\n",
    "        k = 0\n",
    "        for j in range(len(util._get_model_layer_shapes(bidder_model, layer_type=['dense', 'input']))):  # loop over layers including input layer\n",
    "                if j == 0:\n",
    "                    self.upper_bounds_z[bidder][j] = np.array(upper_bound_input).reshape(-1, 1)\n",
    "                    self.upper_bounds_s[bidder][j] = np.array(upper_bound_input).reshape(-1, 1)\n",
    "                else:\n",
    "                    W_plus = np.maximum(Wb_total[k].transpose(), 0)\n",
    "                    W_minus = np.minimum(Wb_total[k].transpose(), 0)\n",
    "                    self.upper_bounds_z[bidder][j] = np.ceil(np.maximum(W_plus @ self.upper_bounds_z[bidder][j-1] + Wb_total[k+1].reshape(-1, 1), 0)).astype(int)   # upper bound for z\n",
    "                    self.upper_bounds_s[bidder][j] = np.ceil(np.maximum(-(W_minus @ self.upper_bounds_z[bidder][j-1] + Wb_total[k+1].reshape(-1, 1)), 0)).astype(int)  # upper bound  for s\n",
    "                    k = k+2\n",
    "        if verbose is True:\n",
    "            logging.debug('Upper Bounds z:')\n",
    "            for k, v in pd.DataFrame({k: pd.Series(l) for k, l in self.upper_bounds_z.items()}).fillna('-').items():\n",
    "                logging.debug(v)\n",
    "            logging.debug('\\nUpper Bounds s:')\n",
    "            for k, v in pd.DataFrame({k: pd.Series(l) for k, l in self.upper_bounds_s.items()}).fillna('-').items():\n",
    "                logging.debug(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_model_weights(model): # torch\n",
    "    nnmodel = model.model\n",
    "    weights = []\n",
    "    for params in nnmodel.parameters():\n",
    "        weights.append(params.detach().cpu().numpy().T)        \n",
    "    return weights\n",
    "\n",
    "def _get_model_layer_shapes(model, layer_type=None):\n",
    "    ''' return layer output shapes instead, \n",
    "        if 'input' is given as desired layer type, insert input dim at the beginning.\n",
    "        assumes torch model '''\n",
    "    # nnmodel = self.Models[key]\n",
    "    nnmodel = model.model\n",
    "    Layer_shapes = []\n",
    "    for i, (name, param) in enumerate(nnmodel.named_parameters()):\n",
    "        if (i==0) and ('input' in layer_type): \n",
    "            Layer_shapes.append(param.shape[1])\n",
    "        if any([x in name for x in layer_type]) and ('bias' in name):\n",
    "            Layer_shapes.append(param.shape[0])\n",
    "    return Layer_shapes\n",
    "\n",
    "def _clean_weights(Wb):\n",
    "    for v in range(0, len(Wb)-2, 2):\n",
    "        Wb[v][abs(Wb[v]) <= 1e-8] = 0\n",
    "        Wb[v+1][abs(Wb[v+1]) <= 1e-8] = 0\n",
    "        zero_rows = np.where(np.logical_and((Wb[v] == 0).all(axis=0), Wb[v+1] == 0))[0]\n",
    "        if len(zero_rows) > 0:\n",
    "            logging.debug('Clean Weights (rows) %s', zero_rows)\n",
    "            Wb[v] = np.delete(Wb[v], zero_rows, axis=1)\n",
    "            Wb[v+1] = np.delete(Wb[v+1], zero_rows)\n",
    "            Wb[v+2] = np.delete(Wb[v+2], zero_rows, axis=0)\n",
    "    return(Wb)\n",
    "\n",
    "bundles = np.random.randint(2, size=(5, 10))\n",
    "values = np.random.randint(100, size=(5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DNNs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-4479642bddf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLCA_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbundles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLCA_NNMIP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDNNs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMIP_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bigM'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'architecture'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dropout'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dropout_prob'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'device'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'regularization_type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'regularization'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DNNs' is not defined"
     ]
    }
   ],
   "source": [
    "#random bundles of 1 or 0 \n",
    "bundles = np.random.randint(2, size=(5, 10))\n",
    "values = np.random.randint(100, size=(5, 1))\n",
    "#print(bundles)\n",
    "#print(values)\n",
    "model = MLCA_NN(X_train = bundles, Y_train=values)\n",
    "\n",
    "X = MLCA_NNMIP(DNNs, L=self.MIP_parameters['bigM'])\n",
    "\n",
    "model.initialize_model(model_parameters={'learning_rate': 0.001, 'architecture': [10, 10, 10], 'dropout': False, 'dropout_prob': 0.2, 'device': 'cpu', 'regularization_type': 'l2', 'regularization': 0.01})\n",
    "\n",
    "model.fit(epochs=10, batch_size=32)\n",
    "\n",
    "models = OrderedDict([(0, model)])\n",
    "\n",
    "mip = MLCA_NNMIP(models, L = 6000)\n",
    "\n",
    "mip.solve_mip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9b69c6f77055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'architecture'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dropout'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dropout_prob'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'device'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'regularization_type'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'regularization'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mgalo_nn_ilp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-9b69c6f77055>\u001b[0m in \u001b[0;36mgalo_nn_ilp\u001b[0;34m(nn_model)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_constraint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_constraint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: -1"
     ]
    }
   ],
   "source": [
    "def galo_nn_ilp(nn_model):\n",
    "\n",
    "    layer = 1\n",
    "    Wb = _clean_weights(_get_model_weights(nn_model))\n",
    "    key = nn_model#do we need this?\n",
    "    i=0 # do we need this\n",
    "    m = Model(name='galo_nn')\n",
    "    \n",
    "    z = {}\n",
    "    q = {}\n",
    "    y = {}\n",
    "    r = m.continuous_var(name='r')\n",
    "    L = 6000\n",
    "    \n",
    "\n",
    "    \n",
    "    m.add_constraint(values[i] - z[-1][0] <= r)\n",
    "    m.add_constraint(z[-1][0] - values[i] <= r)\n",
    "    for i in range(len(values)):    \n",
    "        m.add_constraint(values[i] - z[(i, layer-1, 0)] + L * b[i] <= r)\n",
    "        m.add_constraint(z[(i, layer<-1, 0)] - values[i] + L * (1- b[i]) <= r)\n",
    "    \n",
    "\n",
    "    for v in range(0, len(Wb), 2):\n",
    "        W = Wb[v].transpose()\n",
    "        b = Wb[v + 1]\n",
    "        R, J = W.shape\n",
    "        if v == 0:\n",
    "            z.update({(i, 0, j): m.binary_var(name=\"x({})_{}\".format(i, j)) for j in range(0, J)})\n",
    "        \n",
    "        z.update({(i, layer, r): m.continuous_var(lb=0, name=\"z({},{})_{}\".format(i, layer, r)) for r in range(0, R)})\n",
    "        #s in paper\n",
    "        q.update({(i, layer, r): m.continuous_var(lb=0, name=\"s({},{})_{}\".format(i, layer, r)) for r in range(0, R)}) # it is different from the initial code\n",
    "        y.update({(i, layer, r): m.binary_var(name=\"y({},{})_{}\".format(i, layer, r)) for r in range(0, R)}) # it is different from the initial code\n",
    "\n",
    "\n",
    "        # if self.upper_bounds_z[key][layer][r][0] == 0 TODO\n",
    "        # elif self.upper_bounds_s[key][layer][r][0] == 0: TODO\n",
    "        # else:\n",
    "\n",
    "        for r in range(0, R):#a lot of elif conditions here do it normally for now\n",
    "            print(m.sum(W[r, j]*z[(i, layer-1, j)] for j in range(0, J)) + b[r] == z[(i, layer, r)] - q[(i, layer, r)])\n",
    "            m.add_constraint(ct=(m.sum(W[r, j]*z[(i, layer-1, j)] for j in range(0, J)) + b[r] == z[(i, layer, r)] - q[(i, layer, r)]),\n",
    "                                            ctname=\"AffineCT_Bidder{}_Layer{}_Row{}\".format(i, layer, r))\n",
    "            \n",
    "            \n",
    "            #m.add_constraint(ct=(q[(i, layer, r)] <= nn_model.upper_bounds_s[key][layer][r][0]*y[(i, layer, r)]),\n",
    "\n",
    "\n",
    "            m.add_constraint(0<=z[(i, layer, r)])\n",
    "            m.add_constraint(ct=z[(i, layer, r)] <= y[(i, layer, r)]*L, ctname=\"BinaryCT_Bidder{}_Layer{}_Row{}_Z\".format(i, layer, r))\n",
    "\n",
    "\n",
    "            m.add_constraint(0<=q[(i, layer, r)])\n",
    "            m.add_constraint(ct=q[(i, layer, r)] <= (1-y[(i, layer, r)])*L, ctname=\"BinaryCT_Bidder{}_Layer{}_Row{}_S\".format(i, layer, r))\n",
    "        layer = layer + 1\n",
    "    m.maximize(r)\n",
    "    sol = m.solve()\n",
    "\n",
    "\n",
    "model = MLCA_NN(X_train = bundles, Y_train=values)\n",
    "model.initialize_model(model_parameters={'learning_rate': 0.001, 'architecture': [10, 10, 10], 'dropout': False, 'dropout_prob': 0.2, 'device': 'cpu', 'regularization_type': 'l2', 'regularization': 0.01})\n",
    "model.fit(epochs=10, batch_size=32)\n",
    "galo_nn_ilp(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_model_weights(model): # torch\n",
    "    nnmodel = model.model\n",
    "    weights = []\n",
    "    for params in nnmodel.parameters():\n",
    "        weights.append(params.detach().cpu().numpy().T)        \n",
    "    return weights\n",
    "\n",
    "def _get_model_layer_shapes(model, layer_type=None):\n",
    "    ''' return layer output shapes instead, \n",
    "        if 'input' is given as desired layer type, insert input dim at the beginning.\n",
    "        assumes torch model '''\n",
    "    # nnmodel = self.Models[key]\n",
    "    nnmodel = model.model\n",
    "    Layer_shapes = []\n",
    "    for i, (name, param) in enumerate(nnmodel.named_parameters()):\n",
    "        if (i==0) and ('input' in layer_type): \n",
    "            Layer_shapes.append(param.shape[1])\n",
    "        if any([x in name for x in layer_type]) and ('bias' in name):\n",
    "            Layer_shapes.append(param.shape[0])\n",
    "    return Layer_shapes\n",
    "\n",
    "def _clean_weights(Wb):\n",
    "    for v in range(0, len(Wb)-2, 2):\n",
    "        Wb[v][abs(Wb[v]) <= 1e-8] = 0\n",
    "        Wb[v+1][abs(Wb[v+1]) <= 1e-8] = 0\n",
    "        zero_rows = np.where(np.logical_and((Wb[v] == 0).all(axis=0), Wb[v+1] == 0))[0]\n",
    "        if len(zero_rows) > 0:\n",
    "            logging.debug('Clean Weights (rows) %s', zero_rows)\n",
    "            Wb[v] = np.delete(Wb[v], zero_rows, axis=1)\n",
    "            Wb[v+1] = np.delete(Wb[v+1], zero_rows)\n",
    "            Wb[v+2] = np.delete(Wb[v+2], zero_rows, axis=0)\n",
    "    return(Wb)\n",
    "\n",
    "bundles = np.random.randint(2, size=(5, 10))\n",
    "values = np.random.randint(100, size=(5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0100918  -0.00580223  0.01008647  0.01008843 -0.01007877 -0.010079\n",
      " -0.00973917  0.01008619  0.01000735  0.01009054]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "M = 10\n",
    "#big integer\n",
    "C = 6000\n",
    "L = 6000\n",
    "#while count < 10:\n",
    "model = MLCA_NN(X_train = bundles, Y_train=values)\n",
    "model.initialize_model(model_parameters={'learning_rate': 0.001, 'architecture': [10, 10, 10], 'dropout': False, 'dropout_prob': 0.2, 'device': 'cpu', 'regularization_type': 'l2', 'regularization': 0.01})\n",
    "model.fit(epochs=10, batch_size=32)\n",
    "weights = _get_model_weights(model)\n",
    "layers = _get_model_layer_shapes(model, layer_type=['dense', 'input'])\n",
    "Wb = _clean_weights(weights)\n",
    "#print(Wb[0])\n",
    "#print(layers)\n",
    "#print(weights[5])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(values)):\n",
    "        m  = Model(name='galo_nn')\n",
    "        #create z^k variables\n",
    "        z = {}\n",
    "        y = {}\n",
    "        q = {}\n",
    "        #print(z)\n",
    "        r = m.continuous_var(name='r')\n",
    "        #x = m.binary_var_list(range(M), name='x')\n",
    "        b = m.binary_var_list(range(len(values)), name='b')\n",
    "        \n",
    "        constraints = []\n",
    "        constraints.append(values[i] - z[-1][0] <= r)\n",
    "        constraints.append(z[-1][0] - values[i] <= r)\n",
    "        \n",
    "        for i in range(len(values)):\n",
    "            constraints.append(values[i] - z[-1][0] + C * b[i] <= r)\n",
    "            constraints.append(z[-1][0] - values[i] + C * (1- b[i]) <= r)\n",
    "        \n",
    "\n",
    "    count += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ica_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
