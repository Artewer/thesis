{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import models,layers,regularizers,optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLCA_NN_torch:\n",
    "\n",
    "    def __init__(self, X_train, Y_train, scaler=None):\n",
    "        self.M = X_train.shape[1]  # number of items\n",
    "        self.X_train = X_train  # training set of bundles\n",
    "        self.Y_train = Y_train  # bidder's values for the bundels in X_train\n",
    "        self.X_valid = None   # test/validation set of bundles\n",
    "        self.Y_valid = None  # bidder's values for the bundels in X_valid\n",
    "        self.model_parameters = None  # neural network parameters\n",
    "        self.model = None  # keras model, i.e., the neural network\n",
    "        self.scaler = scaler  # the scaler used for initially scaling the Y_train values\n",
    "        self.history = None  # return value of the model.fit() method from keras\n",
    "        self.loss = None  # return value of the model.fit() method from keras\n",
    "        self.device = 'cpu'\n",
    "\n",
    "    def initialize_model(self, model_parameters):\n",
    "        self.model_parameters = model_parameters\n",
    "        # model parameters is a tuple:(r=regularization_parameters,lr=learning rate for ADAM, dim=number and dimension of hidden layers, dropout=boolean if dropout is used in trainig, dp=dropout rate,epochs=epochs, batch_size=batch_size, regularization_type=regularization_type)\n",
    "        lr = self.model_parameters['learning_rate']\n",
    "        architecture = self.model_parameters['architecture']\n",
    "        dropout = self.model_parameters['dropout']\n",
    "        dp = self.model_parameters['dropout_prob']\n",
    "\n",
    "        architecture = [int(layer) for layer in architecture]  # integer check\n",
    "        number_of_hidden_layers = len(architecture)\n",
    "        dropout = bool(dropout)\n",
    "        # -------------------------------------------------- NN Architecture -------------------------------------------------#\n",
    "        # GET MODEL HERE\n",
    "        # first hidden layer\n",
    "        model = nn.Sequential()\n",
    "        model.add_module('dense_0',nn.Linear(self.M, architecture[0])) \n",
    "        model.add_module('relu_0',nn.ReLU())\n",
    "        if dropout is True: \n",
    "            model.add_module(\"dropout_0\", nn.Dropout(p=dp))\n",
    "\n",
    "        # remaining hidden layer\n",
    "        for k in range(1, number_of_hidden_layers):\n",
    "            model.add_module(f\"dense_{k}\", nn.Linear(architecture[k-1], architecture[k]))\n",
    "            model.add_module(f\"relu_{k}\", nn.ReLU())\n",
    "            if dropout is True:\n",
    "                model.add_module(f\"dropout{k}\", nn.Dropout(p=dp))\n",
    "        # final output layer\n",
    "        model.add_module(f\"dense_{k+1}\", nn.Linear(architecture[k], 1))\n",
    "        model.add_module(f\"relu_{k+1}\", nn.ReLU())        \n",
    "        \n",
    "        # ADAM = adaptive moment estimation a first-order gradient-based optimization algorithm\n",
    "        self.optimizer = optim.Adam(model.parameters(),lr=lr, betas=(0.9, 0.999), weight_decay=0.0, amsgrad=False)\n",
    "        self.criterion = nn.MSELoss(reduction='mean')\n",
    "        self.model = model\n",
    "        logging.debug('Neural Net initialized')\n",
    "\n",
    "        \n",
    "    def __get_reg_loss(self):\n",
    "        regularization_type = self.model_parameters['regularization_type']\n",
    "        r = self.model_parameters['regularization']\n",
    "        w1, w2 = 0,0\n",
    "        # set regularization\n",
    "        if regularization_type == 'l2' or regularization_type is None:\n",
    "            w2 = r\n",
    "        if regularization_type == 'l1':\n",
    "            w1 = r\n",
    "        if regularization_type == 'l1_l2':\n",
    "            w1,w2 = r, r\n",
    "        \n",
    "        l1_regularization, l2_regularization = torch.FloatTensor([0]), torch.FloatTensor([0])\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            l1_regularization += torch.norm(param, 1)**2\n",
    "            l2_regularization += torch.norm(param, 2)**2\n",
    "        \n",
    "        return w1*l1_regularization + w2*l2_regularization\n",
    "\n",
    "        \n",
    "    def fit(self, epochs, batch_size, X_valid=None, Y_valid=None):\n",
    "        # set test set if desired\n",
    "        self.X_valid = X_valid\n",
    "        self.Y_valid = Y_valid\n",
    "\n",
    "        size = self.X_train.shape[0]\n",
    "        N_iter = size//batch_size + int(bool(size%batch_size))\n",
    "\n",
    "\n",
    "        X = torch.FloatTensor(self.X_train).to(self.device)\n",
    "        Y = torch.FloatTensor(self.Y_train).to(self.device)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.epoch_losses = {'train':[], 'val':[]}\n",
    "\n",
    "        for n in range(epochs):\n",
    "\n",
    "            losses = {'train':[], 'val':[]}\n",
    "            indices = np.arange(len(X)) \n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            for i in range(N_iter):\n",
    "                x = X[indices[i*batch_size: (i+1)*batch_size]]\n",
    "                y = Y[indices[i*batch_size: (i+1)*batch_size]]\n",
    "\n",
    "                # Compute prediction and loss\n",
    "                pred = self.model(x)\n",
    "                mse_loss = self.criterion(pred.flatten(), y.flatten())\n",
    "                reg_loss = self.__get_reg_loss()\n",
    "                loss = mse_loss + reg_loss\n",
    "                losses['train'].append(loss.item()*len(x))\n",
    "\n",
    "                # Backpropagation\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            self.epoch_losses['train'].append(np.mean(losses['train']))\n",
    "\n",
    "            if (self.X_valid is not None) and (self.Y_valid is not None):\n",
    "                Xval = torch.FloatTensor(self.X_valid).to(self.device)\n",
    "                Yval = torch.FloatTensor(self.Y_valid).to(self.device)\n",
    "\n",
    "                size_val = self.X_valid.shape[0]\n",
    "                N_iter_val = size_val//batch_size + int(bool(size_val%batch_size))\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for i in range(N_iter_val):\n",
    "                        x = Xval[i*batch_size: (i+1)*batch_size]\n",
    "                        y = Yval[i*batch_size: (i+1)*batch_size]\n",
    "                        pred = self.model(x)\n",
    "                        mse_loss = self.criterion(pred.flatten(), y)\n",
    "                        reg_loss = self.__get_reg_loss()\n",
    "                        loss = mse_loss + reg_loss\n",
    "                        losses['val'].append(loss)\n",
    "\n",
    "                self.epoch_losses['val'].append(np.mean(losses['val']))\n",
    "\n",
    "            \n",
    "#             loss = self.loss_info(batch_size, plot=False)\n",
    "#         return (loss)\n",
    "        tr, val = None, None\n",
    "        tr_orig, val_orig = self.epoch_losses['train'][-1], self.epoch_losses['val']\n",
    "        return ((tr, val, tr_orig, val_orig))\n",
    "\n",
    "    def loss_info(self, batch_size, plot=True, scale=None):\n",
    "        '''\n",
    "        Returns\n",
    "        Scalar test loss (if the model has a single output and no metrics) \n",
    "        or list of scalars (if the model has multiple outputs and/or metrics). \n",
    "        The attribute model.metrics_names will give you the display labels for the scalar outputs.\n",
    "        '''\n",
    "        logging.debug('Model Parameters:')\n",
    "        for k,v in self.model_parameters.items():\n",
    "            logging.debug(k + ': %s', v)\n",
    "        tr = None\n",
    "        tr_orig = None\n",
    "        val = None\n",
    "        val_orig = None\n",
    "        # if scaler attribute was specified\n",
    "        if self.scaler is not None:\n",
    "            logging.debug(' ')\n",
    "            logging.debug('*SCALING*')\n",
    "            logging.debug('---------------------------------------------')\n",
    "            # errors on the training set\n",
    "            tr = self.model.evaluate(x=self.X_train, y=self.Y_train, verbose=0)\n",
    "            tr_orig = float(self.scaler.inverse_transform([[tr]]))\n",
    "            if (self.X_valid is not None) and (self.Y_valid is not None):\n",
    "                # errors on the test set\n",
    "                val = self.model.evaluate(x=self.X_valid, y=self.Y_valid, verbose=0)\n",
    "                val_orig = float(self.scaler.inverse_transform([[val]]))\n",
    "        # data has not been scaled by scaler, i.e., scaler == None\n",
    "        else:\n",
    "            tr_orig = self.model.evaluate(x=self.X_train, y=self.Y_train, verbose=0)\n",
    "            if (self.X_valid is not None) and (self.Y_valid is not None):\n",
    "                val_orig = self.model.evaluate(x=self.X_valid, y=self.Y_valid, verbose=0)\n",
    "        # print errors\n",
    "        if tr is not None:\n",
    "            logging.info('Train Error Scaled %s', tr)\n",
    "        if val is not None:\n",
    "            logging.info('Validation Error Scaled %s', val)\n",
    "        if tr_orig is not None:\n",
    "            logging.info('Train Error Orig. %s', tr_orig)\n",
    "        if val_orig is not None:\n",
    "            logging.info('Validation Error Orig %s', val_orig)\n",
    "        logging.debug('---------------------------------------------')\n",
    "\n",
    "   \n",
    "        return((tr, val, tr_orig, val_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs, batch_size = 10, 30\n",
    "X_valid=None\n",
    "Y_valid=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization_N = 1e-5\n",
    "learning_rate_N = 0.01\n",
    "layer_N = [16,16,16]\n",
    "dropout_N = True\n",
    "dropout_prob_N = 0.05\n",
    "epochs, batch_size = 10, 4\n",
    "regularization_type = 'l1'\n",
    "\n",
    "model_parameters =  OrderedDict([('regularization', regularization_N),\n",
    "                                ('learning_rate', learning_rate_N),\n",
    "                                ('architecture', layer_N),\n",
    "                                ('dropout', dropout_N),\n",
    "                                ('dropout_prob', dropout_prob_N),\n",
    "                                ('epochs', epochs),\n",
    "                                ('batch_size', batch_size),\n",
    "                                ('regularization_type',\n",
    "                                 regularization_type)])\n",
    "\n",
    "\n",
    "nq = 30\n",
    "X_train = np.float32(np.random.randn(nq,18) > .5)\n",
    "Y_train = np.random.rand(nq,1) * 70\n",
    "\n",
    "mlca_nn = MLCA_NN_torch(X_train, Y_train)\n",
    "mlca_nn.initialize_model(model_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('regularization', 1e-05),\n",
       "             ('learning_rate', 0.01),\n",
       "             ('architecture', [16, 16, 16]),\n",
       "             ('dropout', True),\n",
       "             ('dropout_prob', 0.05),\n",
       "             ('epochs', 10),\n",
       "             ('batch_size', 4),\n",
       "             ('regularization_type', 'l1')])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlca_nn.model_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = mlca_nn.fit(epochs=100, batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, 8476.424560546875, [])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (dense_0): Linear(in_features=18, out_features=16, bias=True)\n",
       "  (relu_0): ReLU()\n",
       "  (dropout_0): Dropout(p=0.05, inplace=False)\n",
       "  (dense_1): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (relu_1): ReLU()\n",
       "  (dropout1): Dropout(p=0.05, inplace=False)\n",
       "  (dense_2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (relu_2): ReLU()\n",
       "  (dropout2): Dropout(p=0.05, inplace=False)\n",
       "  (dense_3): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (relu_3): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self.Models[key] = mlca_nn.model\n",
    "nnmodel = mlca_nn.model\n",
    "nnmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 16, 16, 16, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_type=['dense', 'input']\n",
    "Layer_shapes = []\n",
    "for i, (name, param) in enumerate(nnmodel.named_parameters()):\n",
    "    if (i==0) and ('input' in layer_type): \n",
    "        Layer_shapes.append(param.shape[1])\n",
    "    if any([x in name for x in layer_type]) and ('bias' in name):\n",
    "        Layer_shapes.append(param.shape[0])\n",
    "        \n",
    "Layer_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-13-8dfccbef6477>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-8dfccbef6477>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    for name, param in nnmodel.named_parameters():\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "for name, param in nnmodel.named_parameters():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 16, 16, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param.data.shape[0] for name, param in nnmodel.named_parameters() \n",
    " if (any([x in name for x in layer_type])) and ('bias' not in name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer():\n",
    "    def __init__(self,):\n",
    "        self.input\n",
    "        self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "18*16+16*16+16*16+16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kerasmodel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b0d7e7e41cf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mW\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkerasmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'kerasmodel' is not defined"
     ]
    }
   ],
   "source": [
    "for W in kerasmodel.get_weights(): print(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights = []\n",
    "for params in nnmodel.parameters():\n",
    "    weights.append(params.detach().cpu().numpy().T)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPPER BOUND TORCH\n",
    "L = 5000\n",
    "upper_bounds_z = []\n",
    "for layer in Layer_shapes:\n",
    "#     print(layer.output.shape)\n",
    "    upper_bounds_z.append(np.array([L]*layer).reshape(-1, 1))\n",
    "#     print(upper_bounds_z[-1].shape)\n",
    "    \n",
    "# upper_bounds_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upper_bounds_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kerasmodel = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _get_model_layers(self, key, layer_type=None):\n",
    "layer_type=['dense', 'input']\n",
    "Layers = kerasmodel.layers\n",
    "if layer_type is not None:\n",
    "    tmp = [layer.get_config()['name'] for layer in Layers]\n",
    "    Layers = [Layers[i] for i in [tmp.index(s) for s in tmp if any([x in s for x in layer_type])]]\n",
    "Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPPER BOUND KERAS\n",
    "L = 5000\n",
    "upper_bounds_z = []\n",
    "for layer in Layers:\n",
    "    print(layer.output.shape)\n",
    "    upper_bounds_z.append(np.array([L]*layer.output.shape[1]).reshape(-1, 1))\n",
    "#     print(upper_bounds_z[-1].shape)\n",
    "    \n",
    "# upper_bounds_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "for params in mlca_nn.model.parameters():\n",
    "    weights.append(params.detach().cpu().numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in weights:\n",
    "    print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to get weights from keras\n",
    "r = 1e-5\n",
    "lr = 0.01\n",
    "architecture = [16, 16, 16]\n",
    "dropout = True\n",
    "dp = 0.2\n",
    "regularization_type = 'l1'\n",
    "M = 18\n",
    "architecture = [int(layer) for layer in architecture]  # integer check\n",
    "number_of_hidden_layers = len(architecture)\n",
    "dropout = bool(dropout)\n",
    "\n",
    "# define input layer\n",
    "inputs = layers.Input(shape=(X_train.shape[1], ))\n",
    "# set regularization\n",
    "REG = regularizers.l1(r)\n",
    "# first hidden layer\n",
    "x = layers.Dense(architecture[0], kernel_regularizer=REG, bias_regularizer=REG, activation='relu')(inputs)\n",
    "if dropout is True:\n",
    "    x = layers.Dropout(rate=dp)(x)\n",
    "# remaining hidden layer\n",
    "for k in range(1, number_of_hidden_layers):\n",
    "    x = layers.Dense(architecture[k], kernel_regularizer=REG, bias_regularizer=REG, activation='relu')(x)\n",
    "    if dropout is True:\n",
    "        x = layers.Dropout(rate=dp)(x)\n",
    "# final output layer\n",
    "predictions = layers.Dense(1, activation='relu')(x)\n",
    "model = models.Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in model.get_weights():\n",
    "    print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlca_nn.model[0].weight.data.T.numpy().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layers = kerasmodel.layers\n",
    "[layer.get_config()['name'] for layer in Layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.output.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bounds_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [layer.get_config()['name'] for layer in Layers]\n",
    "Layers = [Layers[i] for i in [tmp.index(s) for s in tmp if any([x in s for x in layer_type])]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_v2_torch",
   "language": "python",
   "name": "thesis_v2_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
