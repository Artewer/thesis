{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import models,layers,regularizers,optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  /opt/conda/conda-bld/pytorch_1614378078406/work/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLCA_NN_torch:\n",
    "\n",
    "    def __init__(self, X_train, Y_train, scaler=None):\n",
    "        self.M = X_train.shape[1]  # number of items\n",
    "        self.X_train = X_train  # training set of bundles\n",
    "        self.Y_train = Y_train  # bidder's values for the bundels in X_train\n",
    "        self.X_valid = None   # test/validation set of bundles\n",
    "        self.Y_valid = None  # bidder's values for the bundels in X_valid\n",
    "        self.model_parameters = None  # neural network parameters\n",
    "        self.model = None  # keras model, i.e., the neural network\n",
    "        self.scaler = scaler  # the scaler used for initially scaling the Y_train values\n",
    "        self.history = None  # return value of the model.fit() method from keras\n",
    "        self.loss = None  # return value of the model.fit() method from keras\n",
    "        self.device = 'cpu'\n",
    "\n",
    "    def initialize_model(self, model_parameters):\n",
    "        self.model_parameters = model_parameters\n",
    "        # model parameters is a tuple:(r=regularization_parameters,lr=learning rate for ADAM, dim=number and dimension of hidden layers, dropout=boolean if dropout is used in trainig, dp=dropout rate,epochs=epochs, batch_size=batch_size, regularization_type=regularization_type)\n",
    "        lr = self.model_parameters['learning_rate']\n",
    "        architecture = self.model_parameters['architecture']\n",
    "        dropout = self.model_parameters['dropout']\n",
    "        dp = self.model_parameters['dropout_prob']\n",
    "\n",
    "        architecture = [int(layer) for layer in architecture]  # integer check\n",
    "        number_of_hidden_layers = len(architecture)\n",
    "        dropout = bool(dropout)\n",
    "        # -------------------------------------------------- NN Architecture -------------------------------------------------#\n",
    "        # GET MODEL HERE\n",
    "        # first hidden layer\n",
    "        model = nn.Sequential()\n",
    "        model.add_module('dense_0',nn.Linear(self.M, architecture[0])) \n",
    "        model.add_module('relu_0',nn.ReLU())\n",
    "        if dropout is True: \n",
    "            model.add_module(\"dropout_0\", nn.Dropout(p=dp))\n",
    "\n",
    "        # remaining hidden layer\n",
    "        for k in range(1, number_of_hidden_layers):\n",
    "            model.add_module(f\"dense_{k}\", nn.Linear(architecture[k-1], architecture[k]))\n",
    "            model.add_module(f\"relu_{k}\", nn.ReLU())\n",
    "            if dropout is True:\n",
    "                model.add_module(f\"dropout{k}\", nn.Dropout(p=dp))\n",
    "        # final output layer\n",
    "        model.add_module(f\"dense_{k+1}\", nn.Linear(architecture[k], 1))\n",
    "        model.add_module(f\"relu_{k+1}\", nn.ReLU())        \n",
    "        \n",
    "        # ADAM = adaptive moment estimation a first-order gradient-based optimization algorithm\n",
    "        self.optimizer = optim.Adam(model.parameters(),lr=lr, betas=(0.9, 0.999), weight_decay=0.0, amsgrad=False)\n",
    "        self.criterion = nn.MSELoss(reduction='mean')\n",
    "        self.model = model\n",
    "        logging.debug('Neural Net initialized')\n",
    "\n",
    "        \n",
    "    def __get_reg_loss(self):\n",
    "        regularization_type = self.model_parameters['regularization_type']\n",
    "        r = self.model_parameters['regularization']\n",
    "        w1, w2 = 0,0\n",
    "        # set regularization\n",
    "        if regularization_type == 'l2' or regularization_type is None:\n",
    "            w2 = r\n",
    "        if regularization_type == 'l1':\n",
    "            w1 = r\n",
    "        if regularization_type == 'l1_l2':\n",
    "            w1,w2 = r, r\n",
    "        \n",
    "        l1_regularization, l2_regularization = torch.FloatTensor([0]), torch.FloatTensor([0])\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            l1_regularization += torch.norm(param, 1)**2\n",
    "            l2_regularization += torch.norm(param, 2)**2\n",
    "        \n",
    "        return w1*l1_regularization + w2*l2_regularization\n",
    "\n",
    "        \n",
    "    def fit(self, epochs, batch_size, X_valid=None, Y_valid=None):\n",
    "        # set test set if desired\n",
    "        self.X_valid = X_valid\n",
    "        self.Y_valid = Y_valid\n",
    "\n",
    "        size = self.X_train.shape[0]\n",
    "        N_iter = size//batch_size + int(bool(size%batch_size))\n",
    "\n",
    "\n",
    "        X = torch.FloatTensor(self.X_train).to(self.device)\n",
    "        Y = torch.FloatTensor(self.Y_train).to(self.device)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.epoch_losses = {'train':[], 'val':[]}\n",
    "\n",
    "        for n in range(epochs):\n",
    "\n",
    "            losses = {'train':[], 'val':[]}\n",
    "            indices = np.arange(len(X)) \n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            for i in range(N_iter):\n",
    "                x = X[indices[i*batch_size: (i+1)*batch_size]]\n",
    "                y = Y[indices[i*batch_size: (i+1)*batch_size]]\n",
    "\n",
    "                # Compute prediction and loss\n",
    "                pred = self.model(x)\n",
    "                mse_loss = self.criterion(pred, y)\n",
    "                reg_loss = self.__get_reg_loss()\n",
    "                loss = mse_loss + reg_loss\n",
    "                losses['train'].append(loss.item()*len(x))\n",
    "\n",
    "                # Backpropagation\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            self.epoch_losses['train'].append(np.mean(losses['train']))\n",
    "\n",
    "            if (self.X_valid is not None) and (self.Y_valid is not None):\n",
    "                Xval = torch.FloatTensor(self.X_valid).to(self.device)\n",
    "                Yval = torch.FloatTensor(self.Y_valid).to(self.device)\n",
    "\n",
    "                size_val = self.X_valid.shape[0]\n",
    "                N_iter_val = size_val//batch_size + int(bool(size_val%batch_size))\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for i in range(N_iter_val):\n",
    "                        x = Xval[i*batch_size: (i+1)*batch_size]\n",
    "                        y = Yval[i*batch_size: (i+1)*batch_size]\n",
    "                        pred = self.model(x)\n",
    "                        mse_loss = self.criterion(pred, y)\n",
    "                        reg_loss = self.__get_reg_loss()\n",
    "                        loss = mse_loss + reg_loss\n",
    "                        losses['val'].append(loss)\n",
    "\n",
    "                self.epoch_losses['val'].append(np.mean(losses['val']))\n",
    "\n",
    "            \n",
    "#             loss = self.loss_info(batch_size, plot=False)\n",
    "#         return (loss)\n",
    "        tr, val = None, None\n",
    "        tr_orig, val_orig = self.epoch_losses['train'][-1], self.epoch_losses['val']\n",
    "        return ((tr, val, tr_orig, val_orig))\n",
    "\n",
    "    def loss_info(self, batch_size, plot=True, scale=None):\n",
    "        '''\n",
    "        Returns\n",
    "        Scalar test loss (if the model has a single output and no metrics) \n",
    "        or list of scalars (if the model has multiple outputs and/or metrics). \n",
    "        The attribute model.metrics_names will give you the display labels for the scalar outputs.\n",
    "        '''\n",
    "        logging.debug('Model Parameters:')\n",
    "        for k,v in self.model_parameters.items():\n",
    "            logging.debug(k + ': %s', v)\n",
    "        tr = None\n",
    "        tr_orig = None\n",
    "        val = None\n",
    "        val_orig = None\n",
    "        # if scaler attribute was specified\n",
    "        if self.scaler is not None:\n",
    "            logging.debug(' ')\n",
    "            logging.debug('*SCALING*')\n",
    "            logging.debug('---------------------------------------------')\n",
    "            # errors on the training set\n",
    "            tr = self.model.evaluate(x=self.X_train, y=self.Y_train, verbose=0)\n",
    "            tr_orig = float(self.scaler.inverse_transform([[tr]]))\n",
    "            if (self.X_valid is not None) and (self.Y_valid is not None):\n",
    "                # errors on the test set\n",
    "                val = self.model.evaluate(x=self.X_valid, y=self.Y_valid, verbose=0)\n",
    "                val_orig = float(self.scaler.inverse_transform([[val]]))\n",
    "        # data has not been scaled by scaler, i.e., scaler == None\n",
    "        else:\n",
    "            tr_orig = self.model.evaluate(x=self.X_train, y=self.Y_train, verbose=0)\n",
    "            if (self.X_valid is not None) and (self.Y_valid is not None):\n",
    "                val_orig = self.model.evaluate(x=self.X_valid, y=self.Y_valid, verbose=0)\n",
    "        # print errors\n",
    "        if tr is not None:\n",
    "            logging.info('Train Error Scaled %s', tr)\n",
    "        if val is not None:\n",
    "            logging.info('Validation Error Scaled %s', val)\n",
    "        if tr_orig is not None:\n",
    "            logging.info('Train Error Orig. %s', tr_orig)\n",
    "        if val_orig is not None:\n",
    "            logging.info('Validation Error Orig %s', val_orig)\n",
    "        logging.debug('---------------------------------------------')\n",
    "\n",
    "        # plot results\n",
    "        if plot is True:\n",
    "            # recalculate predicted values for the training set and test set, which are used for the true vs. predicted plot.\n",
    "            Y_hat_train = self.model.predict(x=self.X_train, batch_size=batch_size).flatten()\n",
    "            if (self.X_valid is not None) and (self.Y_valid is not None):\n",
    "                Y_hat_valid = self.model.predict(x=self.X_valid, batch_size=batch_size).flatten()\n",
    "            fig, ax = plt.subplots(1, 2)\n",
    "            plt.subplots_adjust(hspace=0.3)\n",
    "            if scale == 'log':\n",
    "                ax[0].set_yscale('log')\n",
    "            ax[0].plot(self.history.history['loss'])\n",
    "            if (self.X_valid is not None) and (self.Y_valid is not None):\n",
    "                ax[0].plot(self.history.history['val_loss'])\n",
    "            ax[0].set_title('Training vs. Test Loss DNN', fontsize=30)\n",
    "            ax[0].set_ylabel('Mean Absolute Error', fontsize=25)\n",
    "            ax[0].set_xlabel('Number of Epochs', fontsize=25)\n",
    "            ax[0].legend(['Train', 'Test'], loc='upper right', fontsize=20)\n",
    "            ax[1].plot(Y_hat_train, self.Y_train, 'bo')\n",
    "            ax[1].set_ylabel('True Values', fontsize=25)\n",
    "            ax[1].set_xlabel('Predicted Values', fontsize=25)\n",
    "            ax[1].set_title('Prediction Accuracy', fontsize=30)\n",
    "\n",
    "            if (self.X_valid is not None) and (self.Y_valid is not None):\n",
    "                ax[1].plot(Y_hat_valid, self.Y_valid, 'go')\n",
    "            ax[1].legend(['Training Points', 'Test Points'], loc='upper left', fontsize=20)\n",
    "            lims = [\n",
    "                np.min([ax[1].get_xlim(), ax[1].get_ylim()]),  # min of both axes\n",
    "                np.max([ax[1].get_xlim(), ax[1].get_ylim()]),  # max of both axes\n",
    "            ]\n",
    "            ax[1].plot(lims, lims, 'k-')\n",
    "            ax[1].set_aspect('equal')\n",
    "            ax[1].set_xlim(lims)\n",
    "            ax[1].set_ylim(lims)\n",
    "        return((tr, val, tr_orig, val_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs, batch_size = 10, 4\n",
    "X_valid=None\n",
    "Y_valid=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization_N = 1e-5\n",
    "learning_rate_N = 0.01\n",
    "layer_N = [16,16,16]\n",
    "dropout_N = True\n",
    "dropout_prob_N = 0.05\n",
    "epochs, batch_size = 10, 4\n",
    "regularization_type = 'l1'\n",
    "\n",
    "model_parameters =  OrderedDict([('regularization', regularization_N),\n",
    "                                ('learning_rate', learning_rate_N),\n",
    "                                ('architecture', layer_N),\n",
    "                                ('dropout', dropout_N),\n",
    "                                ('dropout_prob', dropout_prob_N),\n",
    "                                ('epochs', epochs),\n",
    "                                ('batch_size', batch_size),\n",
    "                                ('regularization_type',\n",
    "                                 regularization_type)])\n",
    "\n",
    "\n",
    "nq = 120\n",
    "X_train = np.float32(np.random.randn(nq,18) > .5)\n",
    "Y_train = np.random.rand(nq,1) * 70\n",
    "\n",
    "mlca_nn = MLCA_NN_torch(X_train, Y_train)\n",
    "mlca_nn.initialize_model(model_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = mlca_nn.fit(epochs=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (dense_0): Linear(in_features=18, out_features=16, bias=True)\n",
       "  (relu_0): ReLU()\n",
       "  (dropout_0): Dropout(p=0.05, inplace=False)\n",
       "  (dense_1): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (relu_1): ReLU()\n",
       "  (dropout1): Dropout(p=0.05, inplace=False)\n",
       "  (dense_2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (relu_2): ReLU()\n",
       "  (dropout2): Dropout(p=0.05, inplace=False)\n",
       "  (dense_3): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (relu_3): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self.Models[key] = mlca_nn.model\n",
    "nnmodel = mlca_nn.model\n",
    "nnmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 16, 16, 16, 1]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_type=['dense', 'input']\n",
    "Layer_shapes = []\n",
    "for i, (name, param) in enumerate(nnmodel.named_parameters()):\n",
    "    if (i==0) and ('input' in layer_type): \n",
    "        Layer_shapes.append(param.shape[1])\n",
    "    if any([x in name for x in layer_type]) and ('bias' in name):\n",
    "        Layer_shapes.append(param.shape[0])\n",
    "        \n",
    "Layer_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in nnmodel.named_parameters():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 16, 16, 1]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[param.data.shape[0] for name, param in nnmodel.named_parameters() \n",
    " if (any([x in name for x in layer_type])) and ('bias' not in name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dense_3', 'bias']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer():\n",
    "    def __init__(self,):\n",
    "        self.input\n",
    "        self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "816"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "18*16+16*16+16*16+16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 16)\n",
      "(16,)\n",
      "(16, 16)\n",
      "(16,)\n",
      "(16, 16)\n",
      "(16,)\n",
      "(16, 1)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "for W in kerasmodel.get_weights(): print(W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weights = []\n",
    "for params in nnmodel.parameters():\n",
    "    weights.append(params.detach().cpu().numpy().T)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.1134294], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPPER BOUND TORCH\n",
    "L = 5000\n",
    "upper_bounds_z = []\n",
    "for layer in Layer_shapes:\n",
    "#     print(layer.output.shape)\n",
    "    upper_bounds_z.append(np.array([L]*layer).reshape(-1, 1))\n",
    "#     print(upper_bounds_z[-1].shape)\n",
    "    \n",
    "# upper_bounds_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000]]),\n",
       " array([[5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000]]),\n",
       " array([[5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000]]),\n",
       " array([[5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000]]),\n",
       " array([[5000]])]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_bounds_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "kerasmodel = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f4d4404c8d0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f4c78f3aba8>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f4c78f3af98>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f4c78eb44a8>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f4c78f3af28>]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def _get_model_layers(self, key, layer_type=None):\n",
    "layer_type=['dense', 'input']\n",
    "Layers = kerasmodel.layers\n",
    "if layer_type is not None:\n",
    "    tmp = [layer.get_config()['name'] for layer in Layers]\n",
    "    Layers = [Layers[i] for i in [tmp.index(s) for s in tmp if any([x in s for x in layer_type])]]\n",
    "Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 18)\n",
      "(?, 16)\n",
      "(?, 16)\n",
      "(?, 16)\n",
      "(?, 1)\n"
     ]
    }
   ],
   "source": [
    "# UPPER BOUND KERAS\n",
    "L = 5000\n",
    "upper_bounds_z = []\n",
    "for layer in Layers:\n",
    "    print(layer.output.shape)\n",
    "    upper_bounds_z.append(np.array([L]*layer.output.shape[1]).reshape(-1, 1))\n",
    "#     print(upper_bounds_z[-1].shape)\n",
    "    \n",
    "# upper_bounds_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "for params in mlca_nn.model.parameters():\n",
    "    weights.append(params.detach().cpu().numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.18929148e-01,  4.53559905e-02,  4.42951053e-01,\n",
       "          5.55481936e-04,  3.13948505e-02, -1.66309386e-01,\n",
       "         -8.80541503e-01,  6.90855622e-01,  1.48354948e-01,\n",
       "          1.71940997e-01,  3.95700708e-02,  1.50353982e-04,\n",
       "          5.05199470e-02,  2.92447209e-01,  3.57873440e-02,\n",
       "          8.62426981e-02],\n",
       "        [ 2.42102489e-01, -2.72588711e-03, -9.22238111e-01,\n",
       "          1.79898925e-04, -1.94031432e-01,  1.21537137e+00,\n",
       "         -1.65109360e+00, -6.84855163e-01,  1.37793422e-01,\n",
       "          7.46035576e-02,  2.55230904e-01,  5.16394328e-04,\n",
       "         -4.51084346e-01, -8.23684752e-01,  1.76347092e-01,\n",
       "          9.52642560e-02],\n",
       "        [ 1.17326513e-01,  1.11802608e-01,  1.40774712e-01,\n",
       "         -4.73475666e-06,  2.12679937e-01, -7.26036057e-02,\n",
       "          2.85818964e-01, -1.20832086e-01,  2.90042832e-02,\n",
       "          1.26340449e-01,  1.50541186e-01,  4.04810387e-04,\n",
       "          2.73408413e-01, -5.45950457e-02,  1.80974558e-01,\n",
       "          2.40132481e-01],\n",
       "        [ 4.91330564e-01,  4.63267475e-01,  1.44709721e-01,\n",
       "         -5.07604273e-04,  2.36272186e-01,  4.67937887e-01,\n",
       "         -5.01221381e-02,  6.38405681e-01,  4.44041997e-01,\n",
       "          4.95959818e-01,  5.20951748e-01, -6.23307016e-04,\n",
       "          3.52432013e-01, -9.16679084e-01,  4.88213539e-01,\n",
       "          4.61169720e-01],\n",
       "        [ 7.38477111e-02, -1.97515443e-01,  2.81019598e-01,\n",
       "         -2.93003337e-04,  3.68581922e-03, -6.31432652e-01,\n",
       "         -2.47200996e-01,  9.55654979e-02, -1.03614209e-02,\n",
       "          1.85650121e-02, -9.33392160e-03,  6.43236213e-04,\n",
       "          1.66063458e-01,  4.43453163e-01, -1.90251004e-02,\n",
       "         -6.46550730e-02],\n",
       "        [ 6.75854832e-02,  1.23352423e-01,  2.83545643e-01,\n",
       "          1.48175727e-03,  1.30720362e-01, -6.73659801e-01,\n",
       "         -2.78731018e-01, -3.70910257e-01, -3.63475606e-02,\n",
       "          5.77766076e-02, -5.50072640e-02, -1.96445850e-04,\n",
       "          2.10309237e-01,  7.70571470e-01, -4.73762639e-02,\n",
       "          6.21245019e-02],\n",
       "        [-3.40806156e-01, -1.87493786e-01,  1.59448728e-01,\n",
       "          2.01837945e-04,  1.84566919e-02, -9.27151442e-01,\n",
       "         -1.24226743e-02,  6.47278726e-01, -1.56412140e-01,\n",
       "         -1.76741764e-01, -9.92491916e-02, -5.37782558e-04,\n",
       "          1.95362315e-01, -4.73719180e-01,  2.92923301e-02,\n",
       "         -1.08454034e-01],\n",
       "        [-1.56749830e-01, -9.25198942e-02,  1.87523812e-01,\n",
       "         -1.53579749e-03, -9.45562795e-02,  4.20409262e-01,\n",
       "         -1.70875943e+00, -5.61117679e-02, -1.77300036e-01,\n",
       "         -1.55770570e-01, -8.76310989e-02, -7.00216915e-04,\n",
       "          3.92651111e-01,  4.55496639e-01, -2.13111475e-01,\n",
       "          1.46173025e-02],\n",
       "        [ 1.74779713e-01,  2.37001568e-01,  4.18885887e-01,\n",
       "          1.73672888e-04,  1.94848686e-01, -2.32032371e+00,\n",
       "          4.48913664e-01,  7.05919027e-01,  1.55076876e-01,\n",
       "          2.40283221e-01,  2.39927560e-01,  8.38087581e-04,\n",
       "          2.08873644e-01,  2.09853187e-01,  2.82116354e-01,\n",
       "          2.59295911e-01],\n",
       "        [ 1.17556803e-01, -2.76491731e-01, -3.49091679e-01,\n",
       "         -5.28070494e-04, -6.88828453e-02, -2.38779381e-01,\n",
       "          2.57363439e-01, -6.65243745e-01,  2.53264755e-01,\n",
       "         -1.38460159e-01,  2.70102024e-01,  7.12140114e-04,\n",
       "         -6.91988945e-01,  1.57149881e-01, -3.25665921e-01,\n",
       "          1.45358801e-01],\n",
       "        [-1.59417108e-01, -1.42315075e-01,  5.70810847e-02,\n",
       "         -1.28618529e-04, -1.21753760e-01,  3.26151550e-01,\n",
       "          6.11982763e-01,  1.13968223e-01, -1.70234725e-01,\n",
       "         -1.61307707e-01, -5.35707548e-02, -5.13371197e-04,\n",
       "         -1.11441147e-02, -6.50104284e-01, -1.58921763e-01,\n",
       "         -1.74658835e-01],\n",
       "        [ 3.69228125e-02,  2.08745301e-01, -1.49649128e-01,\n",
       "         -3.63407307e-04,  3.38091664e-02, -8.89034808e-01,\n",
       "          1.36688545e-01,  6.00021899e-01,  9.26653855e-03,\n",
       "          2.70080231e-02,  1.01709284e-01, -6.81494770e-04,\n",
       "          3.15237135e-01, -3.89462501e-01,  1.59044310e-01,\n",
       "          1.06258743e-01],\n",
       "        [-2.48928428e-01, -4.53088552e-01,  1.60421524e-02,\n",
       "          4.33043868e-04, -2.92813480e-01,  7.70798028e-01,\n",
       "          8.28797370e-02, -7.45484352e-01, -3.33385229e-01,\n",
       "         -3.48258764e-01, -4.67784733e-01, -1.75227539e-03,\n",
       "         -2.16287300e-01,  2.25163057e-01, -5.21990776e-01,\n",
       "         -3.63687724e-01],\n",
       "        [ 8.69392902e-02,  1.45480305e-01,  8.56229588e-02,\n",
       "         -1.05685415e-03,  1.34109393e-01,  1.90868564e-02,\n",
       "         -1.95079327e+00,  6.10000432e-01,  7.75010362e-02,\n",
       "          3.00071687e-01,  1.27991095e-01,  6.29724353e-04,\n",
       "          1.82822242e-01,  1.54456541e-01,  2.19714418e-01,\n",
       "          3.04393113e-01],\n",
       "        [-2.93985397e-01, -2.12512404e-01, -2.71293521e-01,\n",
       "          6.02092012e-04, -4.56045242e-03,  6.81241572e-01,\n",
       "          2.03738943e-01,  4.85820472e-01, -2.99291641e-01,\n",
       "         -2.14053512e-01, -1.48796305e-01, -1.85835292e-03,\n",
       "         -1.43627807e-01, -1.84522188e+00, -6.47142828e-02,\n",
       "          3.85204554e-02],\n",
       "        [-2.26689830e-01, -3.16588044e-01, -3.23551685e-01,\n",
       "          1.96618366e-05, -1.87587559e-01,  1.27710700e-01,\n",
       "          4.01774347e-02, -1.76406689e-02, -1.52005717e-01,\n",
       "         -1.76301464e-01, -3.15287821e-02, -1.47446262e-05,\n",
       "         -8.46303344e-01,  3.05991799e-01, -3.69119108e-01,\n",
       "         -8.08814242e-02],\n",
       "        [ 2.79568970e-01,  3.19895387e-01, -1.97128296e-01,\n",
       "          6.93551323e-04,  3.56225297e-02, -4.68802869e-01,\n",
       "          1.78432405e-01, -2.02751517e-01,  2.82047123e-01,\n",
       "          2.23633394e-01,  3.84017736e-01,  4.35168884e-04,\n",
       "         -1.49377495e-01, -2.41070420e-01,  3.08177888e-01,\n",
       "          1.93974301e-01],\n",
       "        [-2.61510968e-01, -2.67509401e-01, -2.21722141e-01,\n",
       "         -3.04086460e-03, -2.25284532e-01,  1.33540913e-01,\n",
       "          4.48019445e-01, -2.59667844e-01, -2.62571871e-01,\n",
       "         -1.95529714e-01, -1.39606163e-01, -5.71266690e-04,\n",
       "         -4.00123775e-01, -7.34816134e-01, -1.24468580e-01,\n",
       "          1.62875298e-02]], dtype=float32),\n",
       " array([ 0.33365765,  0.33539915,  0.25184596, -0.10975648,  0.40473315,\n",
       "         0.11609482,  0.5209587 , -0.36153567,  0.34001258,  0.19154614,\n",
       "         0.30480036, -0.06867611,  0.27425897,  0.47121596,  0.25171164,\n",
       "         0.3538578 ], dtype=float32),\n",
       " array([[ 8.03827122e-02,  1.07562598e-02, -8.25879872e-02,\n",
       "          9.24111009e-02,  1.15557938e-04, -9.19529617e-01,\n",
       "          8.82946253e-02,  5.68166859e-02, -1.14299897e-02,\n",
       "         -2.02270001e-01,  5.56671545e-02, -5.43948300e-02,\n",
       "          1.80179533e-02,  9.32956487e-02,  8.68357942e-02,\n",
       "         -1.40349880e-01],\n",
       "        [ 1.69586465e-01,  1.41507402e-01,  3.29865292e-02,\n",
       "          1.28323376e-01,  1.32282221e-04, -8.72853220e-01,\n",
       "          1.29221857e-01,  9.46042910e-02, -2.16979701e-02,\n",
       "         -1.42201334e-01,  1.63127691e-01, -2.43037581e-01,\n",
       "          3.28873545e-02,  1.32667944e-01,  9.81278792e-02,\n",
       "          4.87032644e-02],\n",
       "        [ 1.32092729e-01,  1.64210662e-01,  1.36650071e-01,\n",
       "          2.37957194e-01, -2.05784163e-04, -8.87075663e-01,\n",
       "          1.99934617e-01,  2.21660852e-01, -3.80285792e-02,\n",
       "          6.86982870e-02,  2.27901638e-01, -1.46940842e-01,\n",
       "          1.67223245e-01,  2.62270749e-01,  1.76284775e-01,\n",
       "          2.06561968e-01],\n",
       "        [ 8.09159246e-05, -1.34985894e-04, -3.26246140e-04,\n",
       "          1.72207598e-04,  1.58596120e-03, -1.66624400e-03,\n",
       "         -3.06526126e-05, -6.02578293e-05,  7.22608529e-04,\n",
       "         -3.53368116e-04, -5.12014783e-04, -2.53174314e-03,\n",
       "          1.59055577e-04, -2.39651243e-04, -5.96094644e-04,\n",
       "         -1.46902283e-04],\n",
       "        [-1.00134416e-02, -1.73857994e-02, -1.25568748e-01,\n",
       "          6.91091060e-04, -1.16677293e-05,  4.17093709e-02,\n",
       "          4.54272069e-02,  1.30729005e-02, -2.46538169e-04,\n",
       "         -2.38954335e-01,  6.88346801e-03, -1.05756089e-01,\n",
       "         -6.88880831e-02,  5.06852847e-03,  5.02102263e-02,\n",
       "         -1.61373526e-01],\n",
       "        [ 1.49799645e+00,  1.60665298e+00,  1.85843301e+00,\n",
       "          1.54215264e+00,  5.43848728e-06,  1.40370595e+00,\n",
       "          1.56016302e+00,  1.54038632e+00, -4.59251896e-04,\n",
       "          1.67756617e+00,  1.46814787e+00, -3.45929384e-01,\n",
       "          1.46800745e+00,  1.51056957e+00,  1.59060729e+00,\n",
       "          8.21395338e-01],\n",
       "        [ 8.75888348e-01,  8.54237080e-01,  1.15109372e+00,\n",
       "          9.87249315e-01,  1.78375514e-04,  7.26581752e-01,\n",
       "          8.68978679e-01,  9.88338232e-01,  1.16532203e-04,\n",
       "          1.42102814e+00,  9.88094747e-01, -3.28459114e-01,\n",
       "          8.53336275e-01,  9.75180626e-01,  1.03070962e+00,\n",
       "          1.96243668e+00],\n",
       "        [ 5.03072500e-01,  5.04117608e-01,  5.46701670e-01,\n",
       "          4.50365067e-01,  7.36684888e-06, -1.50350249e+00,\n",
       "          4.41264331e-01,  5.16343236e-01,  1.51981390e-03,\n",
       "          2.33982950e-01,  4.52130347e-01, -1.03394136e-01,\n",
       "          3.86950254e-01,  4.93799925e-01,  4.94985461e-01,\n",
       "          9.63216305e-01],\n",
       "        [ 1.04370005e-01,  2.73485202e-05, -8.38320479e-02,\n",
       "          1.18289143e-01,  6.71510315e-06, -7.45844662e-01,\n",
       "          6.56577721e-02,  6.49330020e-02, -4.49862964e-02,\n",
       "         -1.79532841e-01,  1.20811172e-01, -7.52846822e-02,\n",
       "         -8.10435275e-04,  9.21549127e-02,  1.04522385e-01,\n",
       "         -2.13952035e-01],\n",
       "        [ 1.28686696e-01,  9.95370969e-02,  4.07197289e-02,\n",
       "          1.18440248e-01, -1.65947204e-05, -1.50113821e-01,\n",
       "          1.00299463e-01,  4.78805304e-02,  1.28136162e-04,\n",
       "         -1.56217873e-01,  1.17330156e-01, -9.22046900e-02,\n",
       "          6.13933951e-02,  1.53322905e-01,  1.16573632e-01,\n",
       "          1.24321114e-02],\n",
       "        [ 1.39483690e-01,  1.17938697e-01,  4.69458364e-02,\n",
       "          1.08976394e-01, -9.98744508e-06, -4.15845633e-01,\n",
       "          1.00150026e-01,  1.80763006e-01,  1.16678159e-04,\n",
       "         -7.73954540e-02,  1.63598835e-01, -1.09217040e-01,\n",
       "          4.21679020e-02,  1.70969158e-01,  1.74850896e-01,\n",
       "          9.58939344e-02],\n",
       "        [ 6.54762089e-02,  5.46329394e-02,  1.13087349e-01,\n",
       "          6.64557964e-02, -2.16784654e-03,  3.82687373e-04,\n",
       "          7.05264285e-02,  6.96031451e-02,  1.52362906e-03,\n",
       "          3.11220094e-04,  4.48994637e-02, -2.87609961e-04,\n",
       "          4.71260473e-02,  8.33448172e-02,  9.03691426e-02,\n",
       "          1.38416260e-01],\n",
       "        [ 1.45656571e-01,  1.66352376e-01,  4.94064763e-02,\n",
       "          1.64155975e-01,  7.43632991e-05, -2.25355935e+00,\n",
       "          1.97906807e-01,  2.04459637e-01, -9.39204916e-03,\n",
       "         -6.74054548e-02,  1.87241793e-01, -1.91143140e-01,\n",
       "          1.64735779e-01,  1.88106105e-01,  1.71293706e-01,\n",
       "          2.44716212e-01],\n",
       "        [ 7.97963619e-01,  7.65374839e-01,  1.18597209e+00,\n",
       "          8.24436903e-01,  1.55378060e-04, -1.49257100e+00,\n",
       "          8.00967932e-01,  9.04992759e-01, -5.03818621e-04,\n",
       "          9.88361716e-01,  7.86177695e-01, -1.57147914e-01,\n",
       "          8.59993815e-01,  9.20226038e-01,  8.82556558e-01,\n",
       "          1.55614913e+00],\n",
       "        [ 1.66676939e-01,  9.10944194e-02, -1.02344528e-02,\n",
       "          1.74314260e-01, -3.75569944e-05, -6.45726740e-01,\n",
       "          1.32604077e-01,  1.83688909e-01, -6.76447824e-02,\n",
       "         -9.49463770e-02,  1.46125555e-01, -1.36869341e-01,\n",
       "          1.00652501e-01,  2.01335967e-01,  1.97316483e-01,\n",
       "         -4.60887998e-02],\n",
       "        [ 1.43146202e-01,  1.52065635e-01,  5.51788881e-03,\n",
       "          1.09181568e-01,  4.03496924e-05, -6.34513199e-02,\n",
       "          1.34767056e-01,  1.71576291e-01, -4.95811142e-02,\n",
       "         -1.75595328e-01,  1.86258242e-01, -1.94739345e-02,\n",
       "          9.20898840e-02,  1.68689027e-01,  2.06340447e-01,\n",
       "         -3.07113677e-02]], dtype=float32),\n",
       " array([ 0.34396607,  0.4333843 ,  0.25200766,  0.38490102, -0.0852574 ,\n",
       "         1.0019668 ,  0.49400255,  0.4044724 , -0.18467562,  0.37707266,\n",
       "         0.46005118, -0.17813091,  0.36534658,  0.46937567,  0.5081379 ,\n",
       "        -0.36442554], dtype=float32),\n",
       " array([[-7.14418897e-03,  1.10451935e-03,  1.97056964e-01,\n",
       "          3.56237710e-01,  3.28232497e-01, -2.14440282e-03,\n",
       "         -1.89964689e-04, -8.20390061e-02,  2.64043987e-01,\n",
       "         -8.84918571e-02,  2.91020840e-01, -5.12578350e-04,\n",
       "          3.09933603e-01,  3.19048524e-01,  2.94053167e-01,\n",
       "          3.25591147e-01],\n",
       "        [-7.60207977e-03, -1.66445747e-02,  1.64689943e-01,\n",
       "          3.59221816e-01,  3.07063967e-01, -5.94199693e-04,\n",
       "         -2.83069152e-04, -9.48745757e-02,  3.64931434e-01,\n",
       "         -6.81508183e-02,  2.79909909e-01, -6.70906389e-04,\n",
       "          2.81749934e-01,  3.75978142e-01,  3.35273117e-01,\n",
       "          4.20009553e-01],\n",
       "        [-1.33851524e-02, -6.74933181e-05,  6.27848208e-01,\n",
       "          4.13004339e-01,  4.53754127e-01, -1.18632428e-03,\n",
       "          1.22902286e-03, -4.76256981e-02,  3.55080098e-01,\n",
       "         -7.66528472e-02,  4.80698824e-01,  6.37818710e-04,\n",
       "          3.91679585e-01,  4.12860006e-01,  4.09065247e-01,\n",
       "          4.48267221e-01],\n",
       "        [-6.65092724e-04,  1.88363629e-05,  1.83033094e-01,\n",
       "          3.37496907e-01,  2.99828321e-01, -4.87230631e-04,\n",
       "          1.00286255e-04, -1.30787060e-01,  3.25133860e-01,\n",
       "         -6.41429797e-02,  3.33469599e-01,  2.53004488e-04,\n",
       "          3.46288413e-01,  3.43247086e-01,  3.34544152e-01,\n",
       "          4.13357764e-01],\n",
       "        [ 8.65631911e-04, -2.92257173e-03, -7.77029491e-05,\n",
       "          4.08320193e-05,  5.49065553e-05, -1.00268342e-03,\n",
       "         -1.01872848e-03,  1.70476269e-04, -3.47662972e-05,\n",
       "          6.70237932e-04,  2.56269668e-05, -2.01512803e-03,\n",
       "         -5.35495565e-05,  6.92513422e-05, -1.10755218e-05,\n",
       "          4.10833163e-05],\n",
       "        [ 6.68636784e-02, -1.40609569e-04, -1.56682873e+00,\n",
       "         -1.68076098e+00, -1.54716504e+00,  2.47403677e-03,\n",
       "         -6.50652219e-05, -1.33223855e-03, -1.59593499e+00,\n",
       "         -3.33235785e-03, -1.54081750e+00, -1.93757005e-05,\n",
       "         -1.68163443e+00, -1.51279235e+00, -1.68436694e+00,\n",
       "         -1.71607590e+00],\n",
       "        [-5.69431158e-03, -3.44337635e-02,  1.98524684e-01,\n",
       "          3.52080047e-01,  3.04740608e-01, -1.05236002e-04,\n",
       "         -1.78630900e-04, -1.19026490e-01,  3.18302512e-01,\n",
       "         -7.46159256e-02,  3.49730670e-01, -1.93742628e-04,\n",
       "          3.38268042e-01,  3.46429348e-01,  3.24334770e-01,\n",
       "          4.07440007e-01],\n",
       "        [-8.33696034e-03,  2.29974044e-04,  3.52080494e-01,\n",
       "          3.84111315e-01,  3.49201590e-01, -1.65211479e-03,\n",
       "          2.72739999e-04, -6.75066188e-02,  3.48530293e-01,\n",
       "         -6.69888034e-02,  3.89775276e-01,  3.57473735e-04,\n",
       "          3.71303678e-01,  3.11464190e-01,  3.36735010e-01,\n",
       "          4.03789312e-01],\n",
       "        [-2.15901667e-03, -1.30587898e-04,  9.27695291e-05,\n",
       "          5.39233014e-02,  4.73318808e-02,  2.88357609e-04,\n",
       "          3.21992993e-05, -1.98303699e-03,  8.10298026e-02,\n",
       "          3.81855003e-04, -6.21954561e-04, -1.95614295e-03,\n",
       "          5.64318299e-02,  6.72329590e-02,  8.38640034e-02,\n",
       "          8.73099715e-02],\n",
       "        [-4.75979969e-02, -1.03709885e-04,  7.64534056e-01,\n",
       "          4.78593856e-01,  4.59900409e-01, -7.09348824e-04,\n",
       "         -2.89136264e-03, -1.52186118e-03,  3.92364651e-01,\n",
       "         -5.83758280e-02,  5.31668067e-01, -8.12230632e-04,\n",
       "          4.18457508e-01,  4.29587841e-01,  3.91331077e-01,\n",
       "          4.76212531e-01],\n",
       "        [-5.99239301e-03, -9.20199230e-03,  2.20977440e-01,\n",
       "          3.55045944e-01,  3.35547239e-01, -1.62651495e-03,\n",
       "          2.89429678e-04, -1.12419948e-01,  3.31747651e-01,\n",
       "         -6.01923838e-02,  3.65564674e-01, -2.66557501e-04,\n",
       "          3.24233204e-01,  3.60939533e-01,  3.64319324e-01,\n",
       "          4.14368272e-01],\n",
       "        [-1.13636814e-02, -6.89585519e-04,  3.02754305e-02,\n",
       "          1.83301233e-02, -2.69391775e-01, -2.64251465e-03,\n",
       "         -1.89091195e-04,  1.01713988e-03,  5.11881560e-02,\n",
       "         -1.62357988e-04,  6.07538931e-02,  1.18007744e-03,\n",
       "          5.69136664e-02,  2.07657441e-02,  7.36615509e-02,\n",
       "          6.05935492e-02],\n",
       "        [-8.40795133e-03, -2.75881235e-02,  2.45513663e-01,\n",
       "          3.52723122e-01,  2.83549041e-01,  4.54752066e-04,\n",
       "          1.04561186e-04, -3.40773165e-02,  3.16410840e-01,\n",
       "         -7.50541613e-02,  2.78836459e-01,  7.28903397e-04,\n",
       "          3.02328944e-01,  3.15027177e-01,  3.49108458e-01,\n",
       "          3.57930869e-01],\n",
       "        [-6.42225007e-03, -3.81287740e-04,  3.26854527e-01,\n",
       "          4.00739908e-01,  3.78461629e-01,  6.15736702e-04,\n",
       "         -1.99658243e-05, -1.13409489e-01,  3.84961635e-01,\n",
       "         -8.45011473e-02,  3.29500318e-01,  5.38736349e-04,\n",
       "          3.39052707e-01,  3.32774073e-01,  3.92814547e-01,\n",
       "          4.25534517e-01],\n",
       "        [-5.46423905e-03, -2.65911818e-02,  2.67557412e-01,\n",
       "          4.00480628e-01,  3.34982783e-01,  5.22181042e-04,\n",
       "          1.02888836e-04, -1.30418226e-01,  3.22989613e-01,\n",
       "         -6.82348907e-02,  3.50980997e-01,  3.76689597e-04,\n",
       "          3.84796679e-01,  3.83854240e-01,  3.74001145e-01,\n",
       "          3.78530473e-01],\n",
       "        [-1.68848127e-01,  4.48712417e-05,  1.05523515e+00,\n",
       "          6.17855608e-01,  5.47495723e-01, -2.04821117e-04,\n",
       "          1.94029429e-03,  1.55488681e-03,  5.44459820e-01,\n",
       "          2.49671575e-04,  7.60162055e-01, -8.10290978e-04,\n",
       "          5.90181947e-01,  6.08946621e-01,  5.63365638e-01,\n",
       "          6.37657225e-01]], dtype=float32),\n",
       " array([ 0.12068046, -0.05299498,  0.38333791,  0.783657  ,  0.7341798 ,\n",
       "        -0.06314968, -0.04834173, -0.07505031,  0.75650036, -0.06420332,\n",
       "         0.7107553 , -0.02676123,  0.74215645,  0.7724119 ,  0.75989085,\n",
       "         0.8397189 ], dtype=float32),\n",
       " array([[-9.6799051e-03],\n",
       "        [-7.8555830e-03],\n",
       "        [ 4.2492115e-01],\n",
       "        [ 4.5951664e-01],\n",
       "        [ 3.9934990e-01],\n",
       "        [ 7.5286381e-02],\n",
       "        [ 4.3237237e-06],\n",
       "        [-1.2385196e-01],\n",
       "        [ 4.0956792e-01],\n",
       "        [-6.7286305e-02],\n",
       "        [ 4.1500416e-01],\n",
       "        [-2.9075982e-06],\n",
       "        [ 4.1550630e-01],\n",
       "        [ 4.3260172e-01],\n",
       "        [ 4.2067376e-01],\n",
       "        [ 4.7814018e-01]], dtype=float32),\n",
       " array([0.8905545], dtype=float32)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 16)\n",
      "(16,)\n",
      "(16, 16)\n",
      "(16,)\n",
      "(16, 16)\n",
      "(16,)\n",
      "(16, 1)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "for w in weights:\n",
    "    print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# how to get weights from keras\n",
    "r = 1e-5\n",
    "lr = 0.01\n",
    "architecture = [16, 16, 16]\n",
    "dropout = True\n",
    "dp = 0.2\n",
    "regularization_type = 'l1'\n",
    "M = 18\n",
    "architecture = [int(layer) for layer in architecture]  # integer check\n",
    "number_of_hidden_layers = len(architecture)\n",
    "dropout = bool(dropout)\n",
    "\n",
    "# define input layer\n",
    "inputs = layers.Input(shape=(X_train.shape[1], ))\n",
    "# set regularization\n",
    "REG = regularizers.l1(r)\n",
    "# first hidden layer\n",
    "x = layers.Dense(architecture[0], kernel_regularizer=REG, bias_regularizer=REG, activation='relu')(inputs)\n",
    "if dropout is True:\n",
    "    x = layers.Dropout(rate=dp)(x)\n",
    "# remaining hidden layer\n",
    "for k in range(1, number_of_hidden_layers):\n",
    "    x = layers.Dense(architecture[k], kernel_regularizer=REG, bias_regularizer=REG, activation='relu')(x)\n",
    "    if dropout is True:\n",
    "        x = layers.Dropout(rate=dp)(x)\n",
    "# final output layer\n",
    "predictions = layers.Dense(1, activation='relu')(x)\n",
    "model = models.Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 16)\n",
      "(16,)\n",
      "(16, 16)\n",
      "(16,)\n",
      "(16, 16)\n",
      "(16,)\n",
      "(16, 1)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "for w in model.get_weights():\n",
    "    print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlca_nn.model[0].weight.data.T.numpy().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_1',\n",
       " 'dense',\n",
       " 'dropout',\n",
       " 'dense_1',\n",
       " 'dropout_1',\n",
       " 'dense_2',\n",
       " 'dropout_2',\n",
       " 'dense_3']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Layers = kerasmodel.layers\n",
    "[layer.get_config()['name'] for layer in Layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dimension(1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.output.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000]]),\n",
       " array([[5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000]]),\n",
       " array([[5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000]]),\n",
       " array([[5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000]]),\n",
       " array([[5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000]]),\n",
       " array([[5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000]]),\n",
       " array([[5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000],\n",
       "        [5000]]),\n",
       " array([[5000]])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_bounds_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-ede7e4690fe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mLayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mLayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mLayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-ede7e4690fe6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mLayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mLayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mLayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'layer_type' is not defined"
     ]
    }
   ],
   "source": [
    "tmp = [layer.get_config()['name'] for layer in Layers]\n",
    "Layers = [Layers[i] for i in [tmp.index(s) for s in tmp if any([x in s for x in layer_type])]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_v2_torch",
   "language": "python",
   "name": "thesis_v2_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
