{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import models,layers,regularizers,optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/korhan/miniconda3/envs/thesis_v2_torch/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# how to get weights from keras\n",
    "r = 1e-5\n",
    "lr = 0.01\n",
    "architecture = [32, 32]\n",
    "dropout = True\n",
    "dp = 0.2\n",
    "regularization_type = 'l1'\n",
    "M = 18\n",
    "architecture = [int(layer) for layer in architecture]  # integer check\n",
    "number_of_hidden_layers = len(architecture)\n",
    "dropout = bool(dropout)\n",
    "\n",
    "# define input layer\n",
    "inputs = layers.Input(shape=(X_train.shape[1], ))\n",
    "# set regularization\n",
    "REG = regularizers.l1(r)\n",
    "# first hidden layer\n",
    "x = layers.Dense(architecture[0], kernel_regularizer=REG, bias_regularizer=REG, activation='relu')(inputs)\n",
    "if dropout is True:\n",
    "    x = layers.Dropout(rate=dp)(x)\n",
    "# remaining hidden layer\n",
    "for k in range(1, number_of_hidden_layers):\n",
    "    x = layers.Dense(architecture[k], kernel_regularizer=REG, bias_regularizer=REG, activation='relu')(x)\n",
    "    if dropout is True:\n",
    "        x = layers.Dropout(rate=dp)(x)\n",
    "# final output layer\n",
    "predictions = layers.Dense(1, activation='relu')(x)\n",
    "model = models.Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.calculate_gain('relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLCA_NN_torch:\n",
    "\n",
    "    def __init__(self, X_train, Y_train, scaler=None):\n",
    "        self.M = X_train.shape[1]  # number of items\n",
    "        self.X_train = X_train  # training set of bundles\n",
    "        self.Y_train = Y_train  # bidder's values for the bundels in X_train\n",
    "        self.X_valid = None   # test/validation set of bundles\n",
    "        self.Y_valid = None  # bidder's values for the bundels in X_valid\n",
    "        self.model_parameters = None  # neural network parameters\n",
    "        self.model = None  # keras model, i.e., the neural network\n",
    "        self.scaler = scaler  # the scaler used for initially scaling the Y_train values\n",
    "        self.history = None  # return value of the model.fit() method from keras\n",
    "        self.loss = None  # return value of the model.fit() method from keras\n",
    "        self.device = 'cpu'\n",
    "\n",
    "    def initialize_model(self, model_parameters):\n",
    "        self.model_parameters = model_parameters\n",
    "        # model parameters is a tuple:(r=regularization_parameters,lr=learning rate for ADAM, dim=number and dimension of hidden layers, dropout=boolean if dropout is used in trainig, dp=dropout rate,epochs=epochs, batch_size=batch_size, regularization_type=regularization_type)\n",
    "        lr = self.model_parameters['learning_rate']\n",
    "        architecture = self.model_parameters['architecture']\n",
    "        dropout = self.model_parameters['dropout']\n",
    "        dp = self.model_parameters['dropout_prob']\n",
    "\n",
    "        architecture = [int(layer) for layer in architecture]  # integer check\n",
    "        number_of_hidden_layers = len(architecture)\n",
    "        dropout = bool(dropout)\n",
    "        # -------------------------------------------------- NN Architecture -------------------------------------------------#\n",
    "        # GET MODEL HERE\n",
    "        # first hidden layer\n",
    "        model = nn.Sequential(nn.Linear(self.M, architecture[0]), nn.ReLU() )\n",
    "        if dropout is True: \n",
    "            model.add_module(\"dropout_0\", nn.Dropout(p=dp))\n",
    "\n",
    "        # remaining hidden layer\n",
    "        for k in range(1, number_of_hidden_layers):\n",
    "            model.add_module(f\"linear_{k}\", nn.Linear(architecture[k-1], architecture[k]))\n",
    "            model.add_module(f\"relu_{k}\", nn.ReLU())\n",
    "            if dropout is True:\n",
    "                model.add_module(f\"dropout{k}\", nn.Dropout(p=dp))\n",
    "        # final output layer\n",
    "        model.add_module(f\"linear_{k+1}\", nn.Linear(architecture[k], 1))\n",
    "        model.add_module(f\"relu_{k+1}\", nn.ReLU())        \n",
    "        \n",
    "        # ADAM = adaptive moment estimation a first-order gradient-based optimization algorithm\n",
    "        self.optimizer = optim.Adam(model.parameters(),lr=lr, betas=(0.9, 0.999), weight_decay=0.0, amsgrad=False)\n",
    "        self.criterion = nn.MSELoss(reduction='mean')\n",
    "        self.model = model\n",
    "        logging.debug('Neural Net initialized')\n",
    "\n",
    "        \n",
    "    def __get_reg_loss(self):\n",
    "        regularization_type = self.model_parameters['regularization_type']\n",
    "        r = self.model_parameters['regularization']\n",
    "        w1, w2 = 0,0\n",
    "        # set regularization\n",
    "        if regularization_type == 'l2' or regularization_type is None:\n",
    "            w2 = r\n",
    "        if regularization_type == 'l1':\n",
    "            w1 = r\n",
    "        if regularization_type == 'l1_l2':\n",
    "            w1,w2 = r, r\n",
    "        \n",
    "        l1_regularization, l2_regularization = torch.FloatTensor([0]), torch.FloatTensor([0])\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            l1_regularization += torch.norm(param, 1)**2\n",
    "            l2_regularization += torch.norm(param, 2)**2\n",
    "        \n",
    "        return w1*l1_regularization + w2*l2_regularization\n",
    "\n",
    "        \n",
    "    def fit(self, epochs, batch_size, X_valid=None, Y_valid=None):\n",
    "        # set test set if desired\n",
    "        self.X_valid = X_valid\n",
    "        self.Y_valid = Y_valid\n",
    "\n",
    "        size = self.X_train.shape[0]\n",
    "        N_iter = size//batch_size + int(bool(size%batch_size))\n",
    "\n",
    "\n",
    "        X = torch.FloatTensor(self.X_train).to(self.device)\n",
    "        Y = torch.FloatTensor(self.Y_train).to(self.device)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.epoch_losses = {'train':[], 'val':[]}\n",
    "\n",
    "        for n in range(epochs):\n",
    "\n",
    "            losses = {'train':[], 'val':[]}\n",
    "            indices = np.arange(len(X)) \n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            for i in range(N_iter):\n",
    "                x = X[indices[i*batch_size: (i+1)*batch_size]]\n",
    "                y = Y[indices[i*batch_size: (i+1)*batch_size]]\n",
    "\n",
    "                # Compute prediction and loss\n",
    "                pred = self.model(x)\n",
    "                mse_loss = self.criterion(pred, y)\n",
    "                reg_loss = self.__get_reg_loss()\n",
    "                loss = mse_loss + reg_loss\n",
    "                losses['train'].append(loss.item()*len(x))\n",
    "\n",
    "                # Backpropagation\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            self.epoch_losses['train'].append(np.mean(losses['train']))\n",
    "\n",
    "            if (self.X_valid is not None) and (self.Y_valid is not None):\n",
    "                Xval = torch.FloatTensor(self.X_valid).to(self.device)\n",
    "                Yval = torch.FloatTensor(self.Y_valid).to(self.device)\n",
    "\n",
    "                size_val = self.X_valid.shape[0]\n",
    "                N_iter_val = size_val//batch_size + int(bool(size_val%batch_size))\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for i in range(N_iter_val):\n",
    "                        x = Xval[i*batch_size: (i+1)*batch_size]\n",
    "                        y = Yval[i*batch_size: (i+1)*batch_size]\n",
    "                        pred = self.model(x)\n",
    "                        mse_loss = self.criterion(pred, y)\n",
    "                        reg_loss = self.__get_reg_loss()\n",
    "                        loss = mse_loss + reg_loss\n",
    "                        losses['val'].append(loss)\n",
    "\n",
    "                self.epoch_losses['val'].append(np.mean(losses['val']))\n",
    "\n",
    "            \n",
    "#             loss = self.loss_info(batch_size, plot=False)\n",
    "#         return (loss)\n",
    "        tr, val = None, None\n",
    "        tr_orig, val_orig = self.epoch_losses['train'], self.epoch_losses['val']\n",
    "        return ((tr, val, tr_orig, val_orig))\n",
    "\n",
    "    def loss_info(self, batch_size, plot=True, scale=None):\n",
    "        '''\n",
    "        Returns\n",
    "        Scalar test loss (if the model has a single output and no metrics) \n",
    "        or list of scalars (if the model has multiple outputs and/or metrics). \n",
    "        The attribute model.metrics_names will give you the display labels for the scalar outputs.\n",
    "        '''\n",
    "        logging.debug('Model Parameters:')\n",
    "        for k,v in self.model_parameters.items():\n",
    "            logging.debug(k + ': %s', v)\n",
    "        tr = None\n",
    "        tr_orig = None\n",
    "        val = None\n",
    "        val_orig = None\n",
    "        # if scaler attribute was specified\n",
    "        if self.scaler is not None:\n",
    "            logging.debug(' ')\n",
    "            logging.debug('*SCALING*')\n",
    "            logging.debug('---------------------------------------------')\n",
    "            # errors on the training set\n",
    "            tr = self.model.evaluate(x=self.X_train, y=self.Y_train, verbose=0)\n",
    "            tr_orig = float(self.scaler.inverse_transform([[tr]]))\n",
    "            if (self.X_valid is not None) and (self.Y_valid is not None):\n",
    "                # errors on the test set\n",
    "                val = self.model.evaluate(x=self.X_valid, y=self.Y_valid, verbose=0)\n",
    "                val_orig = float(self.scaler.inverse_transform([[val]]))\n",
    "        # data has not been scaled by scaler, i.e., scaler == None\n",
    "        else:\n",
    "            tr_orig = self.model.evaluate(x=self.X_train, y=self.Y_train, verbose=0)\n",
    "            if (self.X_valid is not None) and (self.Y_valid is not None):\n",
    "                val_orig = self.model.evaluate(x=self.X_valid, y=self.Y_valid, verbose=0)\n",
    "        # print errors\n",
    "        if tr is not None:\n",
    "            logging.info('Train Error Scaled %s', tr)\n",
    "        if val is not None:\n",
    "            logging.info('Validation Error Scaled %s', val)\n",
    "        if tr_orig is not None:\n",
    "            logging.info('Train Error Orig. %s', tr_orig)\n",
    "        if val_orig is not None:\n",
    "            logging.info('Validation Error Orig %s', val_orig)\n",
    "        logging.debug('---------------------------------------------')\n",
    "\n",
    "        # plot results\n",
    "        if plot is True:\n",
    "            # recalculate predicted values for the training set and test set, which are used for the true vs. predicted plot.\n",
    "            Y_hat_train = self.model.predict(x=self.X_train, batch_size=batch_size).flatten()\n",
    "            if (self.X_valid is not None) and (self.Y_valid is not None):\n",
    "                Y_hat_valid = self.model.predict(x=self.X_valid, batch_size=batch_size).flatten()\n",
    "            fig, ax = plt.subplots(1, 2)\n",
    "            plt.subplots_adjust(hspace=0.3)\n",
    "            if scale == 'log':\n",
    "                ax[0].set_yscale('log')\n",
    "            ax[0].plot(self.history.history['loss'])\n",
    "            if (self.X_valid is not None) and (self.Y_valid is not None):\n",
    "                ax[0].plot(self.history.history['val_loss'])\n",
    "            ax[0].set_title('Training vs. Test Loss DNN', fontsize=30)\n",
    "            ax[0].set_ylabel('Mean Absolute Error', fontsize=25)\n",
    "            ax[0].set_xlabel('Number of Epochs', fontsize=25)\n",
    "            ax[0].legend(['Train', 'Test'], loc='upper right', fontsize=20)\n",
    "            ax[1].plot(Y_hat_train, self.Y_train, 'bo')\n",
    "            ax[1].set_ylabel('True Values', fontsize=25)\n",
    "            ax[1].set_xlabel('Predicted Values', fontsize=25)\n",
    "            ax[1].set_title('Prediction Accuracy', fontsize=30)\n",
    "\n",
    "            if (self.X_valid is not None) and (self.Y_valid is not None):\n",
    "                ax[1].plot(Y_hat_valid, self.Y_valid, 'go')\n",
    "            ax[1].legend(['Training Points', 'Test Points'], loc='upper left', fontsize=20)\n",
    "            lims = [\n",
    "                np.min([ax[1].get_xlim(), ax[1].get_ylim()]),  # min of both axes\n",
    "                np.max([ax[1].get_xlim(), ax[1].get_ylim()]),  # max of both axes\n",
    "            ]\n",
    "            ax[1].plot(lims, lims, 'k-')\n",
    "            ax[1].set_aspect('equal')\n",
    "            ax[1].set_xlim(lims)\n",
    "            ax[1].set_ylim(lims)\n",
    "        return((tr, val, tr_orig, val_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epochs, batch_size = 10, 4\n",
    "X_valid=None\n",
    "Y_valid=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization_N = 1e-5\n",
    "learning_rate_N = 0.01\n",
    "layer_N = [16,16,16]\n",
    "dropout_N = True\n",
    "dropout_prob_N = 0.05\n",
    "epochs, batch_size = 10, 4\n",
    "regularization_type = 'l1'\n",
    "\n",
    "model_parameters =  OrderedDict([('regularization', regularization_N),\n",
    "                                ('learning_rate', learning_rate_N),\n",
    "                                ('architecture', layer_N),\n",
    "                                ('dropout', dropout_N),\n",
    "                                ('dropout_prob', dropout_prob_N),\n",
    "                                ('epochs', epochs),\n",
    "                                ('batch_size', batch_size),\n",
    "                                ('regularization_type',\n",
    "                                 regularization_type)])\n",
    "\n",
    "\n",
    "nq = 120\n",
    "X_train = np.float32(np.random.randn(nq,18) > .5)\n",
    "Y_train = np.random.rand(nq,1) * 70\n",
    "\n",
    "mlca_nn = MLCA_NN_torch(X_train, Y_train)\n",
    "mlca_nn.initialize_model(model_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = mlca_nn.fit(epochs=100, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = []\n",
    "for params in mlca_nn.model.parameters():\n",
    "    weights.append(params.detach().cpu().numpy().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-7.01514363e-01, -1.48492217e-01, -3.39565724e-01,\n",
       "         -2.40913138e-01, -4.02501315e-01, -8.49387702e-03,\n",
       "         -3.40017736e-01, -2.27009237e-01, -2.66018510e-01,\n",
       "          1.73972230e-02, -2.40471482e-01,  3.72789532e-01,\n",
       "         -2.99540460e-01, -3.04352820e-01,  1.08604684e-01,\n",
       "          2.25039417e-04],\n",
       "        [-6.83051169e-01, -1.36838034e-01, -3.41687351e-01,\n",
       "         -2.54555374e-01, -3.24204445e-01,  2.25311349e-04,\n",
       "          1.90919772e-01, -7.83938617e-02, -2.07678527e-01,\n",
       "         -9.14849341e-02, -1.89660944e-03, -5.33295497e-02,\n",
       "         -1.79426879e-01, -1.57161489e-01,  4.61566523e-02,\n",
       "         -2.55523773e-05],\n",
       "        [ 1.18651532e-01,  2.68758237e-01,  1.05685450e-01,\n",
       "          1.81071490e-01,  3.50416094e-01, -1.67714328e-01,\n",
       "          8.06426346e-01,  6.54458284e-01,  2.25920528e-01,\n",
       "          2.44494930e-01, -2.63020508e-02, -8.34300816e-01,\n",
       "          3.68928611e-01,  2.78017581e-01, -6.97353005e-01,\n",
       "          7.99542002e-04],\n",
       "        [ 4.60448205e-01,  3.08208793e-01,  8.26034546e-02,\n",
       "          2.24152729e-01,  1.20752960e-01, -8.67158473e-02,\n",
       "         -1.38435602e-01,  1.04954578e-01,  2.45321944e-01,\n",
       "          3.49800557e-01,  3.83974202e-02,  2.28083819e-01,\n",
       "          1.95905194e-01,  2.22888052e-01, -1.00891042e+00,\n",
       "         -4.64865414e-04],\n",
       "        [-6.99967444e-01, -1.58148527e-01, -1.67079359e-01,\n",
       "         -1.00579910e-01, -1.51827410e-01, -1.47603109e-01,\n",
       "         -2.13874340e-01, -6.55419648e-01, -3.14044744e-01,\n",
       "         -9.32253376e-02, -5.28902225e-02, -2.06520826e-01,\n",
       "         -2.32279405e-01, -1.83660612e-01, -4.13128585e-01,\n",
       "          3.80570709e-04],\n",
       "        [ 4.50217366e-01, -1.97447240e-01, -3.22677314e-01,\n",
       "         -9.12423506e-02, -7.84755945e-02, -8.63938481e-02,\n",
       "          3.46185178e-01,  4.66075987e-01, -1.68103963e-01,\n",
       "         -5.90992928e-01,  8.85211676e-02,  4.13253993e-01,\n",
       "         -2.06983387e-01, -7.08204955e-02, -1.41302574e+00,\n",
       "         -1.22203492e-04],\n",
       "        [-8.39802250e-02,  4.19162691e-01,  9.73606259e-02,\n",
       "          1.04089014e-01,  1.72789916e-02, -1.06555775e-01,\n",
       "          6.33722693e-02, -3.41771133e-02,  1.63479179e-01,\n",
       "          4.91143882e-01,  2.36698344e-01,  5.04317991e-02,\n",
       "          4.26620603e-01,  8.70614946e-02,  5.32730639e-01,\n",
       "          1.89162121e-04],\n",
       "        [ 7.96240494e-02, -2.53931612e-01, -3.20152313e-01,\n",
       "         -1.87475443e-01, -3.44986796e-01, -1.05849989e-01,\n",
       "         -3.82250905e-01, -5.93968153e-01, -1.90073699e-01,\n",
       "         -2.82507688e-01, -6.10722959e-01,  6.18880749e-01,\n",
       "         -2.89906532e-01, -3.92523438e-01, -6.60775959e-01,\n",
       "         -1.54624030e-03],\n",
       "        [-2.64264554e-01,  4.13896799e-01,  3.54756266e-01,\n",
       "          3.04008782e-01,  1.31271452e-01, -8.09703633e-05,\n",
       "          7.41859913e-01,  1.39617696e-01,  4.37182158e-01,\n",
       "          6.83969200e-01, -2.92684793e-01, -8.01347494e-01,\n",
       "          3.53898376e-01,  3.31703514e-01, -7.60816708e-02,\n",
       "         -2.01066054e-04],\n",
       "        [-1.11982703e+00, -4.34511751e-01, -2.32013077e-01,\n",
       "         -3.24511051e-01, -2.13484898e-01, -1.17241874e-01,\n",
       "          2.43637525e-02, -3.71706486e-01, -1.54592812e-01,\n",
       "         -2.39400595e-01, -4.61454779e-01,  6.22886956e-01,\n",
       "         -5.22674441e-01, -4.33970124e-01,  1.90601006e-01,\n",
       "          6.58293662e-04],\n",
       "        [-2.15463325e-01,  1.62756667e-01,  4.37063351e-02,\n",
       "          1.36516392e-01,  2.93707419e-02, -1.16896294e-01,\n",
       "         -4.80000585e-01, -7.88617581e-02,  8.67369771e-03,\n",
       "          1.38344511e-01,  1.17674202e-01,  4.09233391e-01,\n",
       "         -9.16840509e-02, -4.89790477e-02,  6.13989234e-01,\n",
       "          5.40809822e-04],\n",
       "        [ 3.82217258e-01, -2.47205317e-01, -4.28301930e-01,\n",
       "         -2.61179060e-01, -1.52720273e-01, -1.27527853e-02,\n",
       "         -1.15159428e+00,  8.58912617e-02, -6.56800866e-02,\n",
       "         -1.28581062e-01, -7.78669119e-01, -4.31873083e-01,\n",
       "          2.07979139e-02,  2.92156674e-02,  2.90186346e-01,\n",
       "          2.52125115e-04],\n",
       "        [ 6.47186816e-01,  5.44088669e-02,  1.45017669e-01,\n",
       "          2.48296671e-02,  1.82568625e-01, -8.12325254e-02,\n",
       "          3.08041334e-01, -9.54068899e-02,  1.18212417e-01,\n",
       "         -3.23554546e-01, -3.16160321e-01, -4.28585112e-01,\n",
       "         -5.64503968e-02, -5.92397638e-02, -2.13948637e-01,\n",
       "          9.96638089e-04],\n",
       "        [ 5.21523058e-01,  2.17953056e-01,  3.87544632e-01,\n",
       "          2.80226082e-01,  3.54752168e-02, -1.65630654e-01,\n",
       "          6.32095158e-01,  5.72302416e-02,  1.03699090e-02,\n",
       "          8.84410292e-02, -2.04541013e-01, -3.27515751e-01,\n",
       "          2.12212935e-01,  1.02481805e-01,  4.37458575e-01,\n",
       "          1.59375719e-04],\n",
       "        [ 3.09447527e-01,  1.00248449e-01,  1.56326398e-01,\n",
       "          3.82927805e-02,  2.19539646e-02, -1.21846981e-01,\n",
       "          5.20750046e-01,  6.21388137e-01,  3.63993365e-03,\n",
       "          2.19939917e-01, -9.40742567e-02, -1.70205259e+00,\n",
       "          1.17521718e-01,  8.26067179e-02, -3.67721952e-02,\n",
       "          1.83756114e-04],\n",
       "        [ 3.90364602e-02,  1.32633850e-01,  7.85820484e-02,\n",
       "          9.95260254e-02,  2.43992120e-01, -1.61673963e-01,\n",
       "         -1.92348659e-01, -1.15810201e-01,  3.31506938e-01,\n",
       "          9.19077545e-02,  1.27834618e-01, -2.70037204e-01,\n",
       "          1.17913581e-01,  4.20584306e-02,  7.71155000e-01,\n",
       "          1.15656655e-03],\n",
       "        [ 4.89095688e-01,  4.51552629e-01,  3.90311509e-01,\n",
       "          2.60268658e-01,  3.22259933e-01, -1.22161739e-01,\n",
       "         -1.64357215e-01,  3.37870926e-01,  3.79806876e-01,\n",
       "          5.10927379e-01, -1.06316678e-01, -9.16358292e-01,\n",
       "          5.16763985e-01,  2.60239929e-01, -1.45374656e+00,\n",
       "          4.37029667e-04],\n",
       "        [-1.63631693e-01,  2.19518960e-01,  5.86651742e-01,\n",
       "          1.71131268e-01,  2.92209804e-01,  1.07287571e-01,\n",
       "          6.43874943e-01,  2.08539203e-01,  2.63651699e-01,\n",
       "          4.77253616e-01,  1.14506285e-03, -4.87211823e-01,\n",
       "          3.30067992e-01,  1.05139256e-01, -1.39478123e+00,\n",
       "          1.59829040e-03]], dtype=float32),\n",
       " array([ 0.11889563,  0.05981809,  0.31254938,  0.19877155,  0.38039705,\n",
       "        -0.16990703, -0.01087529,  0.0074162 ,  0.17376542,  0.21625826,\n",
       "         0.5419073 ,  0.8933773 ,  0.2506991 ,  0.35310486,  0.00289858,\n",
       "        -0.19661641], dtype=float32),\n",
       " array([[ 2.83002330e-04, -2.82254457e-01,  1.13730729e+00,\n",
       "          4.38636065e-01,  6.05898082e-01,  4.99267340e-01,\n",
       "          6.93582356e-01,  5.32051665e-04,  4.84709680e-01,\n",
       "          1.52168784e-03,  4.78307307e-01,  5.07955790e-01,\n",
       "          4.60646063e-01,  6.06783688e-01,  4.44549829e-01,\n",
       "          6.30976558e-01],\n",
       "        [ 5.28469216e-04, -1.84846669e-01,  2.58562416e-01,\n",
       "          8.18615928e-02, -9.70981196e-02,  2.75840191e-03,\n",
       "          1.53569788e-01, -7.09793472e-04,  3.03290814e-01,\n",
       "          1.07158581e-03,  1.43678159e-01, -1.00025743e-01,\n",
       "          1.63804978e-01,  2.05382198e-01,  2.01877147e-01,\n",
       "          1.35332316e-01],\n",
       "        [-1.24127109e-05, -8.93827081e-02,  3.46963592e-02,\n",
       "          3.06198329e-01, -1.60875499e-01,  8.76611099e-02,\n",
       "          2.54128665e-01, -2.94061552e-04,  2.41822287e-01,\n",
       "         -3.66768974e-04,  3.59324887e-02,  3.04023415e-01,\n",
       "          2.51745552e-01,  2.42690265e-01,  1.16478525e-01,\n",
       "          3.65269408e-02],\n",
       "        [-5.26613730e-04, -6.42301813e-02,  8.93658251e-02,\n",
       "          1.04463929e-02, -7.96662197e-02,  4.55823541e-02,\n",
       "          6.53734729e-02,  4.78834438e-04,  7.53864497e-02,\n",
       "         -5.55166625e-05, -1.05473222e-02,  7.17175826e-02,\n",
       "          2.44042769e-01,  1.00763261e-01,  1.01403221e-02,\n",
       "         -3.83510850e-02],\n",
       "        [ 4.65946097e-04, -2.96394825e-01, -3.42570581e-02,\n",
       "          9.66685787e-02, -3.09435818e-02,  5.99470288e-02,\n",
       "          1.29879206e-01,  3.79469566e-04,  2.23035812e-01,\n",
       "          4.59762960e-05, -3.31075490e-02, -1.52165830e-01,\n",
       "          5.52578047e-02,  1.27486140e-01,  5.28531857e-02,\n",
       "         -2.34463951e-03],\n",
       "        [-2.14854977e-03,  4.76859685e-04,  3.56149321e-05,\n",
       "         -1.34907991e-01, -1.06907345e-01, -1.24322936e-01,\n",
       "         -1.23279892e-01,  3.48568108e-04, -9.67347026e-02,\n",
       "          7.00271223e-04, -1.21839635e-01, -1.13090411e-01,\n",
       "         -1.27349406e-01, -1.08199947e-01, -1.39539108e-01,\n",
       "         -1.09407462e-01],\n",
       "        [ 3.39227699e-05, -1.89701661e-01,  7.17182219e-01,\n",
       "          4.65612054e-01,  6.44514084e-01,  5.26331723e-01,\n",
       "          7.40024984e-01,  1.76769739e-03,  5.03257692e-01,\n",
       "         -6.68455206e-04,  4.53016311e-01,  9.02340174e-01,\n",
       "          4.35083091e-01,  8.04708242e-01,  2.81972378e-01,\n",
       "          6.18988156e-01],\n",
       "        [ 6.20569743e-04, -6.10203966e-02,  5.32934368e-01,\n",
       "          4.13817316e-01,  2.87268400e-01,  3.15740436e-01,\n",
       "          2.71179438e-01, -2.55728140e-04,  3.02662224e-01,\n",
       "         -1.24050805e-03,  3.11494023e-01, -1.31135702e-01,\n",
       "          3.04609537e-01,  3.17399144e-01,  3.76652539e-01,\n",
       "          2.97577858e-01],\n",
       "        [ 9.63350758e-05, -7.05327317e-02,  1.57369599e-01,\n",
       "          1.38188109e-01,  1.35293439e-01,  3.55790253e-03,\n",
       "          1.20281130e-01,  6.97814394e-05,  1.26876235e-01,\n",
       "          4.49585496e-05,  2.06359252e-01, -1.59551978e-01,\n",
       "          3.16898078e-01,  5.72621543e-03,  8.23223740e-02,\n",
       "          4.30348851e-02],\n",
       "        [ 2.10120983e-04, -8.80104378e-02,  4.32232052e-01,\n",
       "          3.63824189e-01,  2.14671686e-01,  1.29037589e-01,\n",
       "          3.31029356e-01, -9.62487102e-05,  1.56817034e-01,\n",
       "          1.01166181e-04,  1.08165994e-01, -1.60306096e-01,\n",
       "          2.99873978e-01,  2.94265926e-01,  1.84225425e-01,\n",
       "          2.39227057e-01],\n",
       "        [ 8.91008240e-05, -1.49591058e-01, -8.84589970e-01,\n",
       "         -1.97744071e-01, -3.10588717e-01, -2.28320435e-01,\n",
       "         -3.53113472e-01, -7.14756781e-04, -2.08069041e-01,\n",
       "         -8.50205077e-04, -2.87872910e-01, -4.83431667e-01,\n",
       "         -2.55248249e-01, -3.13967586e-01, -1.53378919e-01,\n",
       "         -1.50914684e-01],\n",
       "        [ 5.96361351e-04, -8.14849790e-03,  9.28697526e-01,\n",
       "          9.60193157e-01,  1.05982530e+00,  1.16753006e+00,\n",
       "          1.03817475e+00,  2.12793821e-04,  1.29123056e+00,\n",
       "          6.85005856e-04,  1.30865550e+00,  1.58781469e+00,\n",
       "          8.30449700e-01,  1.02565897e+00,  9.94555771e-01,\n",
       "          1.06051850e+00],\n",
       "        [ 1.04614272e-04, -3.13032031e-01,  2.56533861e-01,\n",
       "          1.85055420e-01,  2.19112530e-01,  1.79650635e-01,\n",
       "         -8.96698982e-03, -1.02050806e-04,  3.41340959e-01,\n",
       "          4.22881218e-04,  2.18178242e-01, -5.67185104e-01,\n",
       "          1.76690340e-01,  3.53854835e-01,  2.93724835e-01,\n",
       "          2.48845607e-01],\n",
       "        [-2.58213549e-04, -9.17492956e-02,  1.31181508e-01,\n",
       "          1.42221823e-01,  7.69580677e-02,  1.68228909e-01,\n",
       "          9.14050117e-02, -4.56627633e-04,  9.70956087e-02,\n",
       "          3.34056705e-04, -5.24156988e-02, -2.94854432e-01,\n",
       "          5.52803911e-02,  1.28897905e-01,  9.73121598e-02,\n",
       "          3.65488045e-02],\n",
       "        [ 7.71749910e-05,  6.51644834e-04,  1.43598169e-01,\n",
       "          1.60292733e+00,  1.68031490e+00,  1.98026752e+00,\n",
       "          1.64182079e+00,  2.07188641e-04,  2.03381562e+00,\n",
       "          1.42806413e-04,  1.78386831e+00,  1.46862340e+00,\n",
       "          1.66572344e+00,  1.85108387e+00,  1.72592342e+00,\n",
       "          1.66582203e+00],\n",
       "        [ 2.08312413e-04, -4.09346598e-04, -1.03613688e-03,\n",
       "         -3.13851924e-04, -2.75876164e-03,  5.47420117e-04,\n",
       "         -4.22053912e-04,  7.71665014e-04, -1.37745007e-03,\n",
       "         -7.67656602e-04, -2.33719125e-04,  1.97542273e-03,\n",
       "          4.62636031e-04, -6.11491967e-04, -7.81646348e-04,\n",
       "          4.84958640e-04]], dtype=float32),\n",
       " array([-0.00567075, -0.03304219, -0.4310001 ,  0.40784097,  0.3548525 ,\n",
       "         0.46151188,  0.4802055 , -0.04790678,  0.25292483, -0.05506928,\n",
       "         0.50810385,  0.08905131,  0.39667487,  0.50581867,  0.3938521 ,\n",
       "         0.5316853 ], dtype=float32),\n",
       " array([[ 4.41183895e-03, -2.83611310e-03,  1.45035854e-04,\n",
       "          1.76955131e-03, -1.71936423e-04, -7.20213575e-04,\n",
       "          5.14575280e-04, -7.01168756e-05,  1.67872035e-03,\n",
       "         -1.60673831e-03, -6.49604888e-04,  1.97139033e-03,\n",
       "         -2.15505832e-04,  1.74041139e-04, -1.26903621e-03,\n",
       "          3.79223330e-03],\n",
       "        [ 1.33954803e-04,  1.00286558e-01,  5.06261848e-02,\n",
       "          4.33769869e-03,  4.52425778e-02,  1.83071181e-01,\n",
       "          1.26352802e-01, -3.41715320e-04,  1.40350655e-01,\n",
       "          1.35361701e-01, -7.58623413e-04, -7.54660578e-04,\n",
       "         -1.09544635e-04,  1.01998068e-01,  1.00846495e-03,\n",
       "         -4.54085064e-04],\n",
       "        [ 1.03361090e-04,  4.37088549e-01,  3.57641697e-01,\n",
       "          5.29889703e-01,  2.95336783e-01,  4.82066602e-01,\n",
       "          5.29954910e-01, -1.22357788e-03,  6.65698647e-01,\n",
       "          7.93247402e-01,  4.71923500e-03, -1.65824115e-01,\n",
       "         -1.02434829e-01,  6.69671595e-01, -1.45883009e-01,\n",
       "          2.66364194e-04],\n",
       "        [ 4.15985705e-05,  2.94845760e-01,  2.67942071e-01,\n",
       "          2.55256295e-01,  3.38734627e-01,  2.15070710e-01,\n",
       "          3.79278332e-01, -2.02878937e-01,  2.84613490e-01,\n",
       "          2.19780907e-01, -8.52833129e-03, -2.11887076e-01,\n",
       "         -3.54601204e-01,  2.09866434e-01, -1.84580714e-01,\n",
       "         -7.19894245e-02],\n",
       "        [ 1.96727720e-04,  3.21229428e-01,  3.41158390e-01,\n",
       "          4.35616493e-01,  2.83291161e-01,  3.03818554e-01,\n",
       "          5.57900965e-01,  2.57561696e-05,  3.51206750e-01,\n",
       "          3.25927049e-01, -4.70153661e-03, -1.94115475e-01,\n",
       "         -1.42341465e-01,  3.40921909e-01, -2.28523016e-01,\n",
       "         -6.99197650e-02],\n",
       "        [ 4.10943758e-05,  3.31402898e-01,  4.37509209e-01,\n",
       "          3.02391917e-01,  2.65508085e-01,  3.68569165e-01,\n",
       "          5.63618481e-01,  6.98801450e-05,  3.87721509e-01,\n",
       "          2.76019752e-01, -4.77048419e-02, -2.16560125e-01,\n",
       "         -1.51625900e-02,  2.23596051e-01, -1.37470827e-01,\n",
       "         -6.30537644e-02],\n",
       "        [-4.07977641e-05,  3.68039131e-01,  4.62732494e-01,\n",
       "          3.24713320e-01,  4.06139970e-01,  5.41278720e-01,\n",
       "          2.08890751e-01, -3.11865078e-05,  3.79331738e-01,\n",
       "          3.59755367e-01,  8.27554893e-03, -2.04979509e-01,\n",
       "         -8.01927373e-02,  3.15710634e-01, -2.14481324e-01,\n",
       "         -7.26776123e-02],\n",
       "        [-2.57885898e-03,  1.37402688e-03,  1.67060483e-04,\n",
       "         -1.17235992e-03, -3.77326622e-04,  9.59226047e-04,\n",
       "         -1.56693219e-03, -3.88747663e-04, -1.37344818e-03,\n",
       "         -1.38655235e-03,  2.67458824e-03, -2.57975841e-03,\n",
       "         -3.29246162e-04,  2.07856786e-03, -1.21807738e-03,\n",
       "         -5.78478561e-04],\n",
       "        [-1.24748345e-04,  3.05651575e-01,  4.25822735e-01,\n",
       "          3.21210086e-01,  8.72954205e-02,  4.55605119e-01,\n",
       "          4.47211772e-01, -9.20761231e-05,  4.35959816e-01,\n",
       "          3.13629240e-01,  6.63523097e-03, -2.03145608e-01,\n",
       "         -1.38608307e-01,  2.83091694e-01, -2.30230764e-01,\n",
       "         -6.78091198e-02],\n",
       "        [ 1.37916906e-03,  1.30562577e-03, -3.20661580e-04,\n",
       "         -9.45202890e-04,  6.04312168e-04,  1.57537917e-03,\n",
       "         -1.08434900e-03, -5.05958509e-04, -2.99915578e-03,\n",
       "          2.57732975e-03,  4.34565969e-04,  4.87439218e-04,\n",
       "         -9.63100232e-04,  9.64527077e-04,  1.49044069e-03,\n",
       "          1.02345250e-03],\n",
       "        [-1.63601141e-03,  3.90067399e-01,  3.31069350e-01,\n",
       "          4.03213173e-01,  2.52175629e-01,  5.17778337e-01,\n",
       "          5.36000550e-01,  4.98826485e-05,  5.02747416e-01,\n",
       "          4.45771039e-01,  2.43519992e-03, -2.25538865e-01,\n",
       "         -1.54323488e-01,  3.86917442e-01, -1.82664335e-01,\n",
       "         -5.84190115e-02],\n",
       "        [-8.54347949e-04,  7.88080990e-01,  6.14558458e-01,\n",
       "          7.48724341e-01,  5.10840237e-01,  6.36890829e-01,\n",
       "          7.69150198e-01,  3.55834048e-03,  9.71006274e-01,\n",
       "          8.81772399e-01, -2.89132036e-02, -1.72246903e-01,\n",
       "          6.22913183e-04,  9.60232198e-01, -9.82203633e-02,\n",
       "         -2.70053335e-02],\n",
       "        [ 9.76566371e-05,  2.35140562e-01,  3.89949352e-01,\n",
       "          3.00765991e-01,  4.43491101e-01,  3.00438344e-01,\n",
       "          3.36989820e-01, -9.14622992e-02,  2.92479128e-01,\n",
       "          2.47221991e-01, -3.85119878e-02, -2.17558533e-01,\n",
       "         -1.82627559e-01,  2.00549453e-01, -2.42972165e-01,\n",
       "         -6.52510002e-02],\n",
       "        [ 2.33636732e-04,  3.59212667e-01,  3.04498106e-01,\n",
       "          3.71079564e-01,  3.53813678e-01,  2.56592155e-01,\n",
       "          3.88652444e-01,  2.57967386e-05,  4.58146513e-01,\n",
       "          3.52077037e-01,  1.51555091e-02, -2.05425039e-01,\n",
       "         -4.04521793e-01,  3.34208369e-01, -2.23826423e-01,\n",
       "         -7.33215064e-02],\n",
       "        [ 8.00904963e-05,  2.60075063e-01,  3.16725582e-01,\n",
       "          2.94987440e-01,  3.25465649e-01,  1.68070987e-01,\n",
       "          3.66476446e-01, -1.14889264e-01,  2.78097361e-01,\n",
       "          1.86762393e-01, -1.36873676e-02, -1.97229192e-01,\n",
       "         -1.58499211e-01,  1.75489888e-01, -2.23569781e-01,\n",
       "         -6.86272904e-02],\n",
       "        [-1.52885608e-04,  3.40319723e-01,  4.38287705e-01,\n",
       "          3.43112558e-01,  9.81738865e-02,  4.50314343e-01,\n",
       "          5.00482202e-01, -1.20371187e-05,  4.15761262e-01,\n",
       "          3.63112837e-01, -1.03628626e-02, -1.98577955e-01,\n",
       "         -1.21245414e-01,  3.04404169e-01, -2.06593663e-01,\n",
       "         -7.16851056e-02]], dtype=float32),\n",
       " array([-0.05983747,  0.6215499 ,  0.521664  ,  0.6172763 ,  0.70923513,\n",
       "         0.8171809 ,  0.76432127, -0.01987848,  0.87743455,  0.7036823 ,\n",
       "         0.21486394, -0.2168507 , -0.01722327,  0.65421116, -0.26163095,\n",
       "        -0.10888226], dtype=float32),\n",
       " array([[ 9.8258151e-06],\n",
       "        [ 3.6753333e-01],\n",
       "        [ 4.3678409e-01],\n",
       "        [ 4.2350209e-01],\n",
       "        [ 3.5589537e-01],\n",
       "        [ 4.0248963e-01],\n",
       "        [ 4.3266553e-01],\n",
       "        [-5.4073997e-02],\n",
       "        [ 4.3686041e-01],\n",
       "        [ 3.7667239e-01],\n",
       "        [-1.1657304e-02],\n",
       "        [-2.1582958e-01],\n",
       "        [-1.0629010e-01],\n",
       "        [ 3.5106611e-01],\n",
       "        [-2.5332400e-01],\n",
       "        [-4.7734771e-02]], dtype=float32),\n",
       " array([0.916245], dtype=float32)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 16)\n",
      "(16,)\n",
      "(16, 16)\n",
      "(16,)\n",
      "(16, 16)\n",
      "(16,)\n",
      "(16, 1)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "for w in weights:\n",
    "    print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 32)\n",
      "(32,)\n",
      "(32, 32)\n",
      "(32,)\n",
      "(32, 1)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "for w in model.get_weights():\n",
    "    print(w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlca_nn.model[0].weight.data.T.numpy().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_1', 'dense', 'dropout', 'dense_1', 'dropout_1', 'dense_2']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Layers = model.layers\n",
    "[layer.get_config()['name'] for layer in Layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-ede7e4690fe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mLayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mLayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mLayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-ede7e4690fe6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mLayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mLayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mLayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'layer_type' is not defined"
     ]
    }
   ],
   "source": [
    "tmp = [layer.get_config()['name'] for layer in Layers]\n",
    "Layers = [Layers[i] for i in [tmp.index(s) for s in tmp if any([x in s for x in layer_type])]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_v2_torch",
   "language": "python",
   "name": "thesis_v2_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
